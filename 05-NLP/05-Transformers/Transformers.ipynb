{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Механизмы внимания и трансформеры\n","\n","Одним из основных недостатков рекуррентных сетей является то, что все слова в последовательности оказывают одинаковое влияние на результат. Кроме того, для задач sequence-to-sequence, таких как распознавание именованных сущностей и машинный перевод, нам необходимо запомнить всю входную последовательность в единый вектор смысла. В действительности конкретные слова во входной последовательности часто оказывают большее влияние на определённые выходы, чем другие.\n","\n","Рассмотрим задачу sequence-to-sequence, такую, как машинный перевод. Она реализуется двумя рекуррентными сетями, где одна сеть (**кодировщик**) сворачивает входную последовательность в скрытое состояние, а другая, **декодер**, разворачивает это скрытое состояние в результирующую выходную последовательность. Проблема с этим подходом заключается в том, что конечному состоянию сети будет трудно запомнить начало предложения, что приведет к низкому качеству модели на длинных предложениях.\n","\n","**Механизмы внимания** позволяют бороться с этой проблемой. Они представляют собой средство взвешивания контекстуального воздействия каждого входного токена на каждое выходное предсказание RNN. Внимание реализуется путем создания связей между промежуточными состояниями энкодера и декодера. Таким образом, при генерации выходного символа $y_t$, мы будем учитывать все входные скрытые состояния $h_i$, с различными весовыми коэффициентами $\\alpha_{t,i}$. \n","\n","![Image showing an encoder/decoder model with an additive attention layer](images/encoder-decoder-attention.png)\n","\n","(Тут показана модель энкодера-декодера с аддитивным механизмом внимания из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитируемая из [этого сообщения в блоге](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html))\n","\n","Матрица внимания $\\{\\alpha_{i,j}\\}$ будет определять степень, в которой те или иные входные слова влияют нагенерацию данного слова в выходной последовательности. Ниже приведен пример такой матрицы:\n","\n","![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](images/bahdanau-fig3.png)\n","\n","(Рисунок взят из [Богданау и др., 2015](https://arxiv.org/pdf/1409.0473.pdf) (рис.3))\n","\n","Все современные подходы к NLP используют механизмы внимания. Однако добавление внимания значительно увеличивает количество параметров модели, что приводит к проблемам масштабирования. Ключевым ограничением масштабирования RNN является то, что рекуррентный характер моделей затрудняет параллельное обучение моделей на кластерах GPU, в то время как размер модели становится настолько большим, что обучать её на одном процессоре становится слишком долго. В RNN каждый элемент последовательности должен быть обработан в последовательном порядке, что означает, что процесс обучения не может быть легко распараллелен.\n","\n","Внедрение механизмов внимания и преодоление такого ограничения позволило обучать действительно большие модели, такие, как BERT или GPT-3.\n","\n","## Трансформеры\n","\n","В отличие от рекуррентных сетей, которые последовательно передают контекст из одного блока в другой, **модели трансформеров** используют **позиционную кодировку** и **внимание**, одновременно рассматривая всё предложение и вычисляя коэффициент внимания между словами входного предложения, и между входным и выходным предложением.\n","\n","![Animated GIF showing how the evaluations are performed in transformer models.](images/transformer-animated-explanation.gif) \n","\n","Такой процесс, в отличие от последовательного применения рекуррентной сети, может распараллеливаться лучше, что позволяет создавать гораздо более крупные и выразительные языковые модели. \n","\n","Подробнее про трансформерные модели можно почитать [в этой русскоязычной статье](https://sysblok.ru/knowhow/kak-rabotajut-transformery-krutejshie-nejroseti-nashih-dnej/) (или [здесь](http://jalammar.github.io/illustrated-transformer/) на английском языке).\n","\n","Важной идеей трансформерных моделей является идея **внутреннего внимания** (*self-attention*). Например, рассмотрим два предложения:\n","\n","* Chicken did not cross the road because it was too wide\n","* Chicken did not cross the road because it was too tired\n","\n","В первом случае слово *it* относится к дороге, а во втором - к цыплёнку. Энкодер предложения должен учитывать это обстоятельство, и механизм внутреннего внимания как раз помогает построить матрицу взаимного влияния токенов друг на друга в рамках входной последовательности.\n","\n","## Построение простой модели трансформера\n","\n","Keras не содержит встроенного слоя для трансформеров, но мы можем построить свой собственный. Такой процесс неплохо описан [в документации](https://www.tensorflow.org/text/tutorials/transformer).\n","\n","Как и прежде, мы сосредоточимся на текстовой классификации набора данных AG News, но стоит отметить, что трансформерные модели показывают наилучший результат и в существенно более сложных задачах NLP."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","import numpy as np\n","\n","ds_train, ds_test = tfds.load('ag_news_subset').values()\n","\n","def extract_text(x):\n","    return x['title']+' '+x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x),x['label'])"]},{"cell_type":"markdown","metadata":{},"source":["Новые слои в Keras должны наследовать класс `Layer` и реализовывать метод `call`. Начнем с слоя **позиционного эмбеддинга**, который применяется ко входной последовательности. Мы будем использовать [код из официальной документации Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Также предположим, что все входные последовательности приведены к длине `maxlen`."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class TokenAndPositionEmbedding(keras.layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.maxlen = maxlen\n","\n","    def call(self, x):\n","        maxlen = self.maxlen\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x+positions"]},{"cell_type":"markdown","metadata":{},"source":["Этот слой состоит из двух слоев `Embedding`: для кодирования токенов (способом, который мы обсуждали ранее) и позиций токенов. Позиции токенов создаются в виде последовательности натуральных чисел от 0 до `maxlen` с помощью `tf.range`, а затем передаются через слой эмбеддинга. Два результирующих эмбеддинг-вектора складываются, что приводит к тензору размерности `maxlen`$\\times$`embed_dim`.\n","\n","<img src=\"images/pos-embedding.png\" width=\"40%\"/>\n","\n","Отметим, что в более продвинутых трансформерных моделях вместо позиционных эмбеддингов могут использоваться **позиционные кодировки**, представляющие собой некоторые нелинейные функции, не обучаемые, а подбираемые как гиперпараметры.\n","\n","Теперь давайте реализуем трансформерный блок. В качестве входа он воспринимает выход предыдущего блока позиционного эмбеддинга:"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class TransformerBlock(keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n","        self.ffn = keras.Sequential(\n","            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = keras.layers.Dropout(rate)\n","        self.dropout2 = keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{},"source":["Мы используем блок `MultiHeadAttention` для получения вектора внутреннего внимания (*self-attention*) размерности `maxlen`$\\times$`embed_dim` - для этого на вход слою мы подаём одну и ту же последовательность `inputs`. Мы используем приём, аналогичный residual connection в сетях ResNet для стабилизации обучения, смешивая результат внутреннего внимания с входом, и нормализуя его с помощью `LayerNormalizaton`.\n","\n","> **Примечание**: `LayerNormalization` похож на `BatchNormalization`, но он нормализует выходы предыдущего слоя для каждого обучающего образца независимо друг от друга, чтобы привести их к диапазону [-1..1].\n","\n","Выход этого слоя затем пропускается через полносвязную сеть (в нашем случае — двухслойный персептрон), и результат добавляется к конечному выходу (который снова проходит нормализацию).\n","\n","<img src=\"images/transformer-layer.png\" width=\"30%\" />\n","\n","Теперь мы можем определить полную модель трансформера:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","text_vectorization (TextVect (None, 256)               0         \n","_________________________________________________________________\n","token_and_position_embedding (None, 256, 32)           648192    \n","_________________________________________________________________\n","transformer_block (Transform (None, 256, 32)           10656     \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 32)                0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 32)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 20)                660       \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 20)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 4)                 84        \n","=================================================================\n","Total params: 659,592\n","Trainable params: 659,592\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","maxlen = 256\n","vocab_size = 20000\n","\n","model = keras.models.Sequential([\n","    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n","    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n","    TransformerBlock(embed_dim, num_heads, ff_dim),\n","    keras.layers.GlobalAveragePooling1D(),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Dense(20, activation=\"relu\"),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Dense(4, activation=\"softmax\")\n","])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training tokenizer\n","938/938 [==============================] - 438s 464ms/step - loss: 0.5216 - acc: 0.7946 - val_loss: 0.2669 - val_acc: 0.9157\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x2a5aec3cd00>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["print('Training tokenizer')\n","model.layers[0].adapt(ds_train.map(extract_text))\n","model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["## BERT\n","\n","**BERT** (Bidirectional Encoder Representations from Transformers) представляет собой большую многослойную трансформерную сеть с 12 слоями для *BERT-base* и 24 - для *BERT-large*. Модель сначала предварительно обучается на большом корпусе текстовых данных (Википедия + книги) с использованием self-supervised learning (прогнозирование замаскированных слов в предложении). Во время предварительного обучения модель приобретает значительный уровень понимания языка в целом, который затем может быть использован с другими наборами данных для решения специфических задач. Этот процесс называется **трансферное обучение**, и хорошо описан [в официальной документации Keras](https://keras.io/examples/nlp/masked_language_modeling/).\n","\n","Существует множество вариаций предобученных трансформерных архитектур, которые можно использовать для трансферного обучения, включая BERT, DistilBERT, BigBird, OpenGPT3 и другие. \n","\n","Давайте посмотрим, как мы можем использовать предварительно обученную модель BERT для решения нашей традиционной задачи классификации новостей. Мы позаимствуем идею и некоторый код из [официальной документации](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n","\n","Для загрузки предварительно обученных моделей мы будем использовать **Tensorflow hub**. Загрузим векторизатор, специально обученный  для BERT:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import tensorflow_text \n","import tensorflow_hub as hub\n","vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","       dtype=int32)>,\n"," 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]], dtype=int32)>,\n"," 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","       dtype=int32)>}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer(['I love transformers'])"]},{"cell_type":"markdown","metadata":{},"source":["Важно использовать тот же векторизатор, на котором была обучена исходная сеть. Векторизатор BERT возвращает три компонента:\n","* `input_word_ids` - последовательность номеров токенов для входного предложения\n","* `input_mask` показывает, какая часть последовательности содержит фактические входные данные, а какая - padding. Он похож на маску, создаваемую слоем `Masking`\n","* `input_type_ids` используется для более сложных задач языкового моделирования (например, ответы на вопросы), и позволяет указать два входных предложения в одной последовательности.\n","\n","Затем мы можем создать экземпляр BERT:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pooled_output -> (1, 128)\n","encoder_outputs -> 4\n","sequence_output -> (1, 128, 128)\n","default -> (1, 128)\n"]}],"source":["z = bert(vectorizer(['I love transformers']))\n","for i,x in z.items():\n","    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"]},{"cell_type":"markdown","metadata":{},"source":["Смотрим на то, что возвращает сеть BERT:\n","* `pooled_output` является результатом усреднения всех векторов последнего слоя. Вы можете рассматривать его как семантический вектор для всей входной последовательности. Он эквивалентен выходу слоя `GlobalAveragePooling1D` в нашей предыдущей модели.\n","* `sequence_output` - это выход последнего слоя трансформера (соответствует выходу `TransformerBlock` в нашей модели выше)\n","* `encoder_outputs` - это выходы всех промежуточных слоёв сети. Поскольку мы загрузили 4-слойную модель BERT (как вы, вероятно, можете догадаться из названия, которое содержит «4_H»), этот выход содержит 4 тензора. Последний из них совпадает с `sequence_output`.\n","\n","Теперь определим сквозную модель классификации. Мы будем использовать *функциональное определение модели* в Keras. Мы также сделаем веса модели BERT необучаемыми и обучим только окончательный классификатор:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None,)]            0                                            \n","__________________________________________________________________________________________________\n","keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n","                                                                 keras_layer[0][1]                \n","                                                                 keras_layer[0][2]                \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 4,782,981\n","Trainable params: 516\n","Non-trainable params: 4,782,465\n","__________________________________________________________________________________________________\n"]}],"source":["inp = keras.Input(shape=(),dtype=tf.string)\n","x = vectorizer(inp)\n","x = bert(x)\n","x = keras.layers.Dropout(0.1)(x['pooled_output'])\n","out = keras.layers.Dense(4,activation='softmax')(x)\n","model = keras.models.Model(inp,out)\n","bert.trainable = False\n","model.summary()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["Несмотря на то, что обучаемых параметров немного, процесс довольно медленный, потому что экстрактор признаков BERT является весьма сложной моделью. На одной эпохе обучения точность не слишком высока, можно попробовать продолжить обучение, или использовать несколько других приёмов.\n","\n","Давайте попробуем разморозить веса BERT и тренировать их. Это требует очень малой скорости обучения, а также более тщательной стратегии обучения с **разминкой**, с использованием оптимизатора **AdamW**. Мы будем использовать пакет `tf-models-official` для создания оптимизатора:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tf-models-official"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None,)]            0                                            \n","__________________________________________________________________________________________________\n","keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n","                                                                 keras_layer[0][1]                \n","                                                                 keras_layer[0][2]                \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 4,782,981\n","Trainable params: 4,782,980\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from official.nlp import optimization \n","bert.trainable=True\n","model.summary()\n","epochs = 3\n","opt = optimization.create_optimizer(\n","    init_lr=3e-5,\n","    num_train_steps=epochs*len(ds_train),\n","    num_warmup_steps=0.1*epochs*len(ds_train),\n","    optimizer_type='adamw')\n","\n","model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["Как видите, обучение идет довольно медленно - но вы можете поэкспериментировать и обучить модель в течение нескольких эпох (5-10) и посмотреть, сможете ли вы получить лучший результат по сравнению с подходами, которые мы использовали раньше.\n","\n","## Библиотека Huggingface Transformers\n","\n","Другим очень распространенным (и немного более простым) способом использования моделей Transformer является [пакет HuggingFace](https://github.com/huggingface/), который предоставляет своего рода *строительные блоки* для различных задач NLP. Он доступен как для Tensorflow, так и для PyTorch, еще одного очень популярного фреймворка нейронных сетей. \n","\n","> **Примечание**: Ниже мы будем повторять те же шаги обучения модели BERT с использованием другой библиотеки и существенно большей модели. Этот процесс включает в себя довольно длительное обучение, поэтому вы можете просто просмотреть код.\n","\n","Давайте посмотрим, как классификация новостей может быть решена с помощью [Huggingface](http://huggingface.co)."]},{"cell_type":"markdown","metadata":{},"source":["Первое, что нужно сделать - это выбрать модель, которую мы будем использовать. В дополнение к некоторым встроенным моделям, Huggingface содержит [онлайн-репозиторий моделей](https://huggingface.co/models), где вы можете найти гораздо больше предварительно обученных сообществом моделей. Все эти модели можно загрузить и использовать, просто указав имя модели. Все необходимые веса модели будут загружены автоматически.\n","\n","Для загрузки собственных моделей, или моделей, хранящихся на диске, можно указать каталог, который содержит все соответствующие файлы, включая параметры для токенизатора, файл `config.json` с параметрами модели, двоичные веса и т.д.\n","\n","Из имени модели мы можем создать экземпляр как самой модели, так и токенизатора. Начнем с токенизатора:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import transformers\n","\n","# Имя модели для загрузки из интернет\n","bert_model = 'bert-base-uncased' \n","\n","# Для загрузки с диска укажите путь\n","#bert_model = './bert'\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n","\n","MAX_SEQ_LEN = 128\n","PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n","UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"]},{"cell_type":"markdown","metadata":{},"source":["Объект `tokenizer` содержит функцию `encode`, которую можно напрямую использовать для кодирования текста:"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode('Tensorflow is a great framework for NLP')"]},{"cell_type":"markdown","metadata":{},"source":["Мы также можем использовать токенизатор для кодирования последовательности тем способом, который подходит для передачи в модель, т.е. включеная поля `token_ids`, `input_mask` и т.д. Мы также можем указать, что нам нужны тензоры Tensorflow, предоставив аргумент `return_tensors='tf'`:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(['Hello, there'],return_tensors='tf')"]},{"cell_type":"markdown","metadata":{},"source":["В нашем случае мы будем использовать предварительно обученную модель BERT под названием `bert-base-uncased` (из названия можно догадаться, что модель не чувствительна к регистру). \n","\n","При обучении модели нам нужно подать на вход токенизированную последовательность, поэтому опишем конвейер обработки данных. Поскольку `tokenizer.encode` является функцией Python, мы будем использовать `py_function`:"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def process(x):\n","    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n","\n","def process_fn(x):\n","    s = x['title']+' '+x['description']\n","    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n","    e.set_shape(MAX_SEQ_LEN)\n","    return e,x['label']"]},{"cell_type":"markdown","metadata":{},"source":["Теперь мы можем загрузить саму модель с помощью класса `BertForSequenceClassfication` - такая модель уже имеет необходимую архитектуру для классификации, включая финальный классификатор. Вы увидите предупреждающее сообщение о том, что веса финального классификатора не инициализированы, и модель требует обучения - это совершенно нормально, потому что это именно то, что мы собираемся сделать!"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_75 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  3076      \n","=================================================================\n","Total params: 109,485,316\n","Trainable params: 109,485,316\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Как видно из 'summary()', модель содержит почти 110 миллионов параметров! Предположительно, если нам нужна простая задача классификации относительно небольшого набора данных, мы не хотим обучать базовую BERT-модель:"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_75 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  3076      \n","=================================================================\n","Total params: 109,485,316\n","Trainable params: 3,076\n","Non-trainable params: 109,482,240\n","_________________________________________________________________\n"]}],"source":["model.layers[0].trainable = False\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["\n","Теперь мы готовы приступить к обучению!\n","\n","> **Примечание**: Обучение полномасштабной модели BERT может занять очень много времени! Мы лишь покажем начало обучения в демонстративных целях. Если вам интересно попробовать полномасштабное обучение - просто уберите параметры `steps_per_epoch` и `validation_steps` и приготовьтесь ждать!"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.compile('adam','sparse_categorical_crossentropy',['acc'])\n","tf.get_logger().setLevel('ERROR')\n","model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"]},{"cell_type":"markdown","metadata":{},"source":["Если увеличить количество итераций и ждать достаточно долго, обучать модель в течение нескольких эпох, то можно ожидать, что классификация BERT даст нам наилучшую точность! Это связано с тем, что BERT уже довольно хорошо понимает структуру языка, и нам нужно только точно настроить окончательный классификатор. Однако, поскольку BERT является большой моделью, весь процесс обучения занимает много времени и требует серьезных вычислительных мощностей! (GPU, и желательно более одного).\n","\n","> **Примечание:** В нашем примере мы использовали одну из самых маленьких предварительно обученных моделей BERT. Существуют более крупные модели, которые, вероятно, дадут ещё более хорошие результаты."]},{"cell_type":"markdown","metadata":{},"source":["## Выводы\n","\n","В этом разделе мы познакомились с современными архитектурами нейросетей на основе **трансформеров**, применили их для задачи классификации текста. Аналогичным образом модели BERT можно использовать для извлечения сущностей, ответов на вопросы и других задач NLP.\n","\n","Модели трансформеров представляют собой современное состояние NLP, и при возникновении задач обработки текстов, вероятно, стоит сразу экспериментировать с трансформерными моделями. Однако понимание основополагающих принципов рекуррентных нейронных сетей, о которых мы говорили, также чрезвычайно важно для создания новых продвинутых нейросетевых архитектур."]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"}}},"nbformat":4,"nbformat_minor":4}
