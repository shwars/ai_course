{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Механизмы внимания и трансформеры\n","\n","Одним из основных недостатков рекуррентных сетей является то, что все слова в последовательности оказывают одинаковое влияние на результат. Кроме того, для задач sequence-to-sequence, таких как распознавание именованных сущностей и машинный перевод, нам необходимо запомнить всю входную последовательность в единый вектор смысла. В действительности конкретные слова во входной последовательности часто оказывают большее влияние на определённые выходы, чем другие.\n","\n","Рассмотрим задачу sequence-to-sequence, такую, как машинный перевод. Она реализуется двумя рекуррентными сетями, где одна сеть (**кодировщик**) сворачивает входную последовательность в скрытое состояние, а другая, **декодер**, разворачивает это скрытое состояние в результирующую выходную последовательность. Проблема с этим подходом заключается в том, что конечному состоянию сети будет трудно запомнить начало предложения, что приведет к низкому качеству модели на длинных предложениях.\n","\n","**Механизмы внимания** позволяют бороться с этой проблемой. Они представляют собой средство взвешивания контекстуального воздействия каждого входного токена на каждое выходное предсказание RNN. Внимание реализуется путем создания связей между промежуточными состояниями энкодера и декодера. Таким образом, при генерации выходного символа $y_t$, мы будем учитывать все входные скрытые состояния $h_i$, с различными весовыми коэффициентами $\\alpha_{t,i}$. \n","\n","![Image showing an encoder/decoder model with an additive attention layer](images/encoder-decoder-attention.png)\n","\n","(Тут показана модель энкодера-декодера с аддитивным механизмом внимания из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитируемая из [этого сообщения в блоге](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html))\n","\n","Матрица внимания $\\{\\alpha_{i,j}\\}$ будет определять степень, в которой те или иные входные слова влияют нагенерацию данного слова в выходной последовательности. Ниже приведен пример такой матрицы:\n","\n","![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](images/bahdanau-fig3.png)\n","\n","(Рисунок взят из [Богданау и др., 2015](https://arxiv.org/pdf/1409.0473.pdf) (рис.3))\n","\n","Все современные подходы к NLP используют механизмы внимания. Однако добавление внимания значительно увеличивает количество параметров модели, что приводит к проблемам масштабирования. Ключевым ограничением масштабирования RNN является то, что рекуррентный характер моделей затрудняет параллельное обучение моделей на кластерах GPU, в то время как размер модели становится настолько большим, что обучать её на одном процессоре становится слишком долго. В RNN каждый элемент последовательности должен быть обработан в последовательном порядке, что означает, что процесс обучения не может быть легко распараллелен.\n","\n","Внедрение механизмов внимания и преодоление такого ограничения позволило обучать действительно большие модели, такие, как BERT или GPT-3.\n","\n","## Трансформеры\n","\n","В отличие от рекуррентных сетей, которые последовательно передают контекст из одного блока в другой, **модели трансформеров** используют **позиционную кодировку** и **внимание**, одновременно рассматривая всё предложение и вычисляя коэффициент внимания между словами входного предложения, и между входным и выходным предложением.\n","\n","![Animated GIF showing how the evaluations are performed in transformer models.](images/transformer-animated-explanation.gif) \n","\n","Такой процесс, в отличие от последовательного применения рекуррентной сети, может распараллеливаться лучше, что позволяет создавать гораздо более крупные и выразительные языковые модели. \n","\n","Подробнее про трансформерные модели можно почитать [в этой русскоязычной статье](https://sysblok.ru/knowhow/kak-rabotajut-transformery-krutejshie-nejroseti-nashih-dnej/) (или [здесь](http://jalammar.github.io/illustrated-transformer/) на английском языке).\n","\n","Важной идеей трансформерных моделей является идея **внутреннего внимания** (*self-attention*). Например, рассмотрим два предложения:\n","\n","* Chicken did not cross the road because it was too wide\n","* Chicken did not cross the road because it was too tired\n","\n","В первом случае слово *it* относится к дороге, а во втором - к цыплёнку. Энкодер предложения должен учитывать это обстоятельство, и механизм внутреннего внимания как раз помогает построить матрицу взаимного влияния токенов друг на друга в рамках входной последовательности.\n","\n","## Построение простой модели трансформера\n","\n","Keras не содержит встроенного слоя для трансформеров, но мы можем построить свой собственный. Такой процесс неплохо описан [в документации](https://www.tensorflow.org/text/tutorials/transformer).\n","\n","Как и прежде, мы сосредоточимся на текстовой классификации набора данных AG News, но стоит отметить, что трансформерные модели показывают наилучший результат и в существенно более сложных задачах NLP."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Metal device set to: Apple M2\n","\n","systemMemory: 24.00 GB\n","maxCacheSize: 8.00 GB\n","\n"]},{"name":"stderr","output_type":"stream","text":["2022-12-04 11:24:11.221686: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2022-12-04 11:24:11.221804: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","import numpy as np\n","\n","ds_train, ds_test = tfds.load('ag_news_subset').values()\n","\n","def extract_text(x):\n","    return x['title']+' '+x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x),x['label'])"]},{"cell_type":"markdown","metadata":{},"source":["Новые слои в Keras должны наследовать класс `Layer` и реализовывать метод `call`. Начнем с слоя **позиционного эмбеддинга**, который применяется ко входной последовательности. Мы будем использовать [код из официальной документации Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Также предположим, что все входные последовательности приведены к длине `maxlen`."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class TokenAndPositionEmbedding(keras.layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.maxlen = maxlen\n","\n","    def call(self, x):\n","        maxlen = self.maxlen\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x+positions"]},{"cell_type":"markdown","metadata":{},"source":["Этот слой состоит из двух слоев `Embedding`: для кодирования токенов (способом, который мы обсуждали ранее) и позиций токенов. Позиции токенов создаются в виде последовательности натуральных чисел от 0 до `maxlen` с помощью `tf.range`, а затем передаются через слой эмбеддинга. Два результирующих эмбеддинг-вектора складываются, что приводит к тензору размерности `maxlen`$\\times$`embed_dim`.\n","\n","<img src=\"images/pos-embedding.png\" width=\"40%\"/>\n","\n","Теперь давайте реализуем трансформерный блок. В качестве входа он воспринимает выход предыдущего блока позиционного эмбеддинга:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class TransformerBlock(keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n","        self.ffn = keras.Sequential(\n","            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = keras.layers.Dropout(rate)\n","        self.dropout2 = keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{},"source":["Мы используем блок `MultiHeadAttention` для получения вектора внутреннего внимания (*self-attention*) размерности `maxlen`$\\times$`embed_dim` - для этого на вход слою мы подаём одну и ту же последовательность `inputs`. Мы используем приём, аналогичный residual connection в сетях ResNet для стабилизации обучения, смешивая результат внутреннего внимания с входом, и нормализуя его с помощью `LayerNormalizaton`.\n","\n","> **Примечание**: `LayerNormalization` похож на `BatchNormalization`, но он нормализует выходы предыдущего слоя для каждого обучающего образца независимо друг от друга, чтобы привести их к диапазону [-1..1].\n","\n","Выход этого слоя затем пропускается через полносвязную сеть (в нашем случае — двухслойный персептрон), и результат добавляется к конечному выходу (который снова проходит нормализацию).\n","\n","<img src=\"images/transformer-layer.png\" width=\"30%\" />\n","\n","Теперь мы можем определить полную модель трансформера:"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text_vectorization_1 (TextV  (None, 256)              0         \n"," ectorization)                                                   \n","                                                                 \n"," token_and_position_embeddin  (None, 256, 32)          648192    \n"," g_1 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," transformer_block (Transfor  (None, 256, 32)          10656     \n"," merBlock)                                                       \n","                                                                 \n"," global_average_pooling1d (G  (None, 32)               0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 20)                660       \n","                                                                 \n"," dropout_3 (Dropout)         (None, 20)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 84        \n","                                                                 \n","=================================================================\n","Total params: 659,592\n","Trainable params: 659,592\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","maxlen = 256\n","vocab_size = 20000\n","\n","model = keras.models.Sequential([\n","    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n","    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n","    TransformerBlock(embed_dim, num_heads, ff_dim),\n","    keras.layers.GlobalAveragePooling1D(),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Dense(20, activation=\"relu\"),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Dense(4, activation=\"softmax\")\n","])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training tokenizer\n"]},{"name":"stderr","output_type":"stream","text":["2022-12-04 11:45:20.282332: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n","2022-12-04 11:45:20.372871: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"]},{"ename":"NotFoundError","evalue":"Graph execution error:\n\nDetected at node 'StringSplit/stack' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/gn/641gjh5947d6slkxgblfqts40000gn/T/ipykernel_8661/2642904648.py\", line 2, in <module>\n      model.layers[0].adapt(ds_train.map(extract_text))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 467, in adapt\n      super().adapt(data, batch_size=batch_size, steps=steps)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 258, in adapt\n      self._adapt_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step\n      self.update_state(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 470, in update_state\n      self._lookup_layer.update_state(self._preprocess(data))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 575, in _preprocess\n      inputs = tf.strings.split(inputs)\nNode: 'StringSplit/stack'\n2 root error(s) found.\n  (0) NOT_FOUND:  No registered 'ExpandDims' OpKernel for 'GPU' devices compatible with node {{node StringSplit/stack}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, Tdim=DT_INT32, _XlaHasReferenceVars=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n\t.  Registered:  device='XLA_CPU_JIT'; Tdim in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 16005131165644881776, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='DEFAULT'; T in [DT_HALF]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_HALF]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_BFLOAT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_BFLOAT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_FLOAT]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_FLOAT]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_DOUBLE]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_DOUBLE]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT32]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT32]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT8]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT8]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT8]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT8]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_COMPLEX64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_COMPLEX64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_COMPLEX128]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_COMPLEX128]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_BOOL]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_BOOL]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT32]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT32]; Tdim in [DT_INT64]\n  device='CPU'; Tdim in [DT_INT32]\n  device='CPU'; Tdim in [DT_INT64]\n\n\t [[StringSplit/stack]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_adapt_step_754]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining tokenizer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49madapt(ds_train\u001b[39m.\u001b[39;49mmap(extract_text))\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m], optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mfit(ds_train\u001b[39m.\u001b[39mmap(tupelize)\u001b[39m.\u001b[39mbatch(\u001b[39m128\u001b[39m),validation_data\u001b[39m=\u001b[39mds_test\u001b[39m.\u001b[39mmap(tupelize)\u001b[39m.\u001b[39mbatch(\u001b[39m128\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py:467\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m     \u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[39m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m--> 258\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[1;32m    259\u001b[0m         \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m    260\u001b[0m             context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'StringSplit/stack' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/gn/641gjh5947d6slkxgblfqts40000gn/T/ipykernel_8661/2642904648.py\", line 2, in <module>\n      model.layers[0].adapt(ds_train.map(extract_text))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 467, in adapt\n      super().adapt(data, batch_size=batch_size, steps=steps)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 258, in adapt\n      self._adapt_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 123, in adapt_step\n      self.update_state(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 470, in update_state\n      self._lookup_layer.update_state(self._preprocess(data))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 575, in _preprocess\n      inputs = tf.strings.split(inputs)\nNode: 'StringSplit/stack'\n2 root error(s) found.\n  (0) NOT_FOUND:  No registered 'ExpandDims' OpKernel for 'GPU' devices compatible with node {{node StringSplit/stack}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, Tdim=DT_INT32, _XlaHasReferenceVars=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n\t.  Registered:  device='XLA_CPU_JIT'; Tdim in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 16005131165644881776, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='DEFAULT'; T in [DT_HALF]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_HALF]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_BFLOAT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_BFLOAT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_FLOAT]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_FLOAT]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_DOUBLE]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_DOUBLE]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT32]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT32]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT16]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT16]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_UINT8]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_UINT8]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT8]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT8]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_COMPLEX64]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_COMPLEX64]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_COMPLEX128]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_COMPLEX128]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_BOOL]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_BOOL]; Tdim in [DT_INT64]\n  device='DEFAULT'; T in [DT_INT32]; Tdim in [DT_INT32]\n  device='DEFAULT'; T in [DT_INT32]; Tdim in [DT_INT64]\n  device='CPU'; Tdim in [DT_INT32]\n  device='CPU'; Tdim in [DT_INT64]\n\n\t [[StringSplit/stack]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_adapt_step_754]"]}],"source":["print('Training tokenizer')\n","model.layers[0].adapt(ds_train.map(extract_text))\n","model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["## BERT\n","\n","**BERT** (Bidirectional Encoder Representations from Transformers) is a very large multi layer transformer network with 12 layers for *BERT-base*, and 24 for *BERT-large*. The model is first pre-trained on large corpus of text data (WikiPedia + books) using unsupervised training (predicting masked words in a sentence). During pre-training the model absorbs significant level of language understanding which can then be leveraged with other datasets using fine tuning. This process is called **transfer learning**. \n","\n","\n","**BERT** (Bidirectional Encoder Representations from Transformers) представляет собой большую многослойную трансформерную сеть с 12 слоями для *BERT-base* и 24 - для *BERT-large*. Модель сначала предварительно обучается на большом корпусе текстовых данных (Википедия + книги) с использованием self-supervised learning (прогнозирование замаскированных слов в предложении). Во время предварительного обучения модель приобретает значительный уровень понимания языка в целом, который затем может быть использован с другими наборами данных для решения специфических задач. Этот процесс называется **трансферное обучение**, и хорошо описан [в официальной документации Keras](https://keras.io/examples/nlp/masked_language_modeling/).\n","\n","![picture from http://jalammar.github.io/illustrated-bert/](./images/jalammarBERT-language-modeling-masked-lm.png)\n","\n","Существует множество вариаций предобученных трансформерных архитектур, которые можно использовать для трансферного обучения, включая BERT, DistilBERT, BigBird, OpenGPT3 и другие. \n","\n","Давайте посмотрим, как мы можем использовать предварительно обученную модель BERT для решения нашей традиционной задачи классификации новостей. Мы позаимствуем идею и некоторый код из [официальной документации](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n","\n","Для загрузки предварительно обученных моделей мы будем использовать **Tensorflow hub**. Загрузим векторизатор, специально обученный  для BERT:"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow_text'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"]}],"source":["import tensorflow_text \n","import tensorflow_hub as hub\n","vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","       dtype=int32)>,\n"," 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]], dtype=int32)>,\n"," 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n"," array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","       dtype=int32)>}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer(['I love transformers'])"]},{"cell_type":"markdown","metadata":{},"source":["Важно использовать тот же векторизатор, на котором была обучена исходная сеть. Векторизатор BERT возвращает три компонента:\n","* `input_word_ids` - последовательность номеров токенов для входного предложения\n","* `input_mask` показывает, какая часть последовательности содержит фактические входные данные, а какая - padding. Он похож на маску, создаваемую слоем `Masking`\n","* `input_type_ids` используется для более сложных задач языкового моделирования (например, ответы на вопросы), и позволяет указать два входных предложения в одной последовательности.\n","\n","Затем мы можем создать экземпляр BERT:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pooled_output -> (1, 128)\n","encoder_outputs -> 4\n","sequence_output -> (1, 128, 128)\n","default -> (1, 128)\n"]}],"source":["z = bert(vectorizer(['I love transformers']))\n","for i,x in z.items():\n","    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"]},{"cell_type":"markdown","metadata":{},"source":["Смотрим на то, что возвращает сеть BERT:\n","* `pooled_output` является результатом усреднения всех веторов последнего слоя. Вы можете рассматривать его как семантический вектор для всей входной последовательности. Он эквивалентен выходу слоя `GlobalAveragePooling1D` в нашей предыдущей модели.\n","* `sequence_output` - это выход последнего слоя трансформера (соответствует выходу `TransformerBlock` в нашей модели выше)\n","* `encoder_outputs` - это выходы всех промежуточных слоёв сети. Поскольку мы загрузили 4-слойную модель BERT (как вы, вероятно, можете догадаться из названия, которое содержит «4_H»), этот выход содержит 4 тензора. Последний из них совпадает с `sequence_output`.\n","\n","Теперь определим сквозную модель классификации. Мы будем использовать *функциональное определение модели* в Keras. Мы также сделаем веса модели BERT необучаемыми и обучим только окончательный классификатор:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None,)]            0                                            \n","__________________________________________________________________________________________________\n","keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n","                                                                 keras_layer[0][1]                \n","                                                                 keras_layer[0][2]                \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 4,782,981\n","Trainable params: 516\n","Non-trainable params: 4,782,465\n","__________________________________________________________________________________________________\n"]}],"source":["inp = keras.Input(shape=(),dtype=tf.string)\n","x = vectorizer(inp)\n","x = bert(x)\n","x = keras.layers.Dropout(0.1)(x['pooled_output'])\n","out = keras.layers.Dense(4,activation='softmax')(x)\n","model = keras.models.Model(inp,out)\n","bert.trainable = False\n","model.summary()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["Несмотря на то, что обучаемых параметров немного, процесс довольно медленный, потому что экстрактор признаков BERT является весьма сложной моделью. На одной эпохе обучения точность не слишком высока, можно попробовать продолжить обучение, или использовать несколько других приёмов.\n","\n","Давайте попробуем разморозить веса BERT и тренировать их. Это требует очень малой скорости обучения, а также более тщательной стратегии обучения с **разминкой**, с использованием оптимизатора **AdamW**. Мы будем использовать пакет `tf-models-official` для создания оптимизатора:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None,)]            0                                            \n","__________________________________________________________________________________________________\n","keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n","                                                                 keras_layer[0][1]                \n","                                                                 keras_layer[0][2]                \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 4,782,981\n","Trainable params: 4,782,980\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from official.nlp import optimization \n","bert.trainable=True\n","model.summary()\n","epochs = 3\n","opt = optimization.create_optimizer(\n","    init_lr=3e-5,\n","    num_train_steps=epochs*len(ds_train),\n","    num_warmup_steps=0.1*epochs*len(ds_train),\n","    optimizer_type='adamw')\n","\n","model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n","model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"]},{"cell_type":"markdown","metadata":{},"source":["Как видите, обучение идет довольно медленно - но вы можете поэкспериментировать и обучить модель в течение нескольких эпох (5-10) и посмотреть, сможете ли вы получить лучший результат по сравнению с подходами, которые мы использовали раньше.\n","\n","## Библиотека Huggingface Transformers\n","\n","Другим очень распространенным (и немного более простым) способом использования моделей Transformer является [пакет HuggingFace](https://github.com/huggingface/), который предоставляет своего рода *строительные блоки* для различных задач NLP. Он доступен как для Tensorflow, так и для PyTorch, еще одного очень популярного фреймворка нейронных сетей. \n","\n","> **Примечание**: Ниже мы будем повторять те же шаги обучения модели BERT с использованием другой библиотеки и существенно большей модели. Таким образом, процесс включает в себя довольно длительное обучение, поэтому вы можете просто просмотреть код.\n","\n","Let's see how our problem can be solved using [Huggingface Transformers](http://huggingface.co).\n","\n","Давайте посмотрим, как наша проблема может быть решена с помощью [Трансформеры Huggingface] (http://huggingface.co)."]},{"cell_type":"markdown","metadata":{},"source":["First thing we need to do is to chose the model that we will be using. In addition to some built-in models, Huggingface contains an [online model repository](https://huggingface.co/models), where you can find a lot more pre-trained models by the community. All of those models can be loaded and used just by providing a model name. All required binary files for the model would automatically be downloaded.\n","\n","\n","Первое, что нам нужно сделать, это выбрать модель, которую мы будем использовать. В дополнение к некоторым встроенным моделям, Huggingface содержит [онлайн-репозиторий моделей] (https://huggingface.co/models), где вы можете найти гораздо больше предварительно обученных моделей сообществом. Все эти модели можно загрузить и использовать, просто указав имя модели. Все необходимые двоичные файлы для модели будут загружены автоматически.\n","\n","At certain times you would need to load your own models, in which case you can specify the directory that contains all relevant files, including parameters for tokenizer, `config.json` file with model parameters, binary weights, etc.\n","\n","\n","В определенное время вам нужно будет загрузить свои собственные модели, и в этом случае вы можете указать каталог, который содержит все соответствующие файлы, включая параметры для токенизатора, файл 'config.json' с параметрами модели, двоичные веса и т. Д.\n","\n","From model name, we can instantiate both the model and the tokenizer. Let's start with a tokenizer:\n","\n","Из имени модели мы можем создать экземпляр как модели, так и токенизатора. Начнем с токенизатора:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import transformers\n","\n","# To load the model from Internet repository using model name. \n","# Use this if you are running from your own copy of the notebooks\n","bert_model = 'bert-base-uncased' \n","\n","# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n","# prepared all required files for you.\n","#bert_model = './bert'\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n","\n","MAX_SEQ_LEN = 128\n","PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n","UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"]},{"cell_type":"markdown","metadata":{},"source":["The `tokenizer` object contains the `encode` function that can be directly used to encode text:\n","\n","Объект 'tokenizer' содержит функцию 'encode', которую можно напрямую использовать для кодирования текста:"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode('Tensorflow is a great framework for NLP')"]},{"cell_type":"markdown","metadata":{},"source":["We can also use tokenizer to encode a sequence is a way suitable for passing to the model, i.e. including `token_ids`, `input_mask` fields, etc. We can also specify that we want Tensorflow tensors by providing `return_tensors='tf'` argument:\n","\n","Мы также можем использовать токенизатор для кодирования последовательности — способа, подходящего для передачи в модель, т.е. включения полей «token_ids», «input_mask» и т.д. Мы также можем указать, что нам нужны тензоры Tensorflow, предоставив аргумент 'return_tensors='tf'':"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(['Hello, there'],return_tensors='tf')"]},{"cell_type":"markdown","metadata":{},"source":["In our case, we will be using pre-trained BERT model called `bert-base-uncased`. *Uncased* indicates that the model in case-insensitive. \n","\n","\n","В нашем случае мы будем использовать предварительно обученную модель BERT под названием 'bert-base-uncased'. *Без регистра* указывает, что модель не чувствительна к регистру. \n","\n","When training the model, we need to provide tokenized sequence as input, and thus we will design data processing pipeline. Since `tokenizer.encode` is a Python function, we will use the same approach as in the last unit with calling it using `py_function`:\n","\n","При обучении модели нам нужно предоставить токенизированную последовательность в качестве входных данных, и таким образом мы спроектируем конвейер обработки данных. Поскольку 'tokenizer.encode' является функцией Python, мы будем использовать тот же подход, что и в последнем блоке, с вызовом его с помощью 'py_function':"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def process(x):\n","    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n","\n","def process_fn(x):\n","    s = x['title']+' '+x['description']\n","    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n","    e.set_shape(MAX_SEQ_LEN)\n","    return e,x['label']"]},{"cell_type":"markdown","metadata":{},"source":["Now we can load the actual model using `BertForSequenceClassfication` package. This ensures that our model already has a required architecture for classification, including final classifier. You will see warning message stating that weights of the final classifier are not initialized, and model would require pre-training - that is perfectly okay, because it is exactly what we are about to do!\n","\n","Теперь мы можем загрузить фактическую модель с помощью пакета 'BertForSequenceClassfication'. Это гарантирует, что наша модель уже имеет необходимую архитектуру для классификации, включая окончательный классификатор. Вы увидите предупреждающее сообщение о том, что веса конечного классификатора не инициализируются, а модель потребует предварительной подготовки - это совершенно нормально, потому что это именно то, что мы собираемся сделать!"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_75 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  3076      \n","=================================================================\n","Total params: 109,485,316\n","Trainable params: 109,485,316\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see from `summary()`, the model contains almost 110 million parameters! Presumably, if we want simple classification task on relatively small dataset, we do not want to train the BERT base layer:\n","\n","Как видно из 'summary()', модель содержит почти 110 миллионов параметров! Предположительно, если нам нужна простая задача классификации относительно небольшого набора данных, мы не хотим обучать базовый слой BERT:"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_75 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  3076      \n","=================================================================\n","Total params: 109,485,316\n","Trainable params: 3,076\n","Non-trainable params: 109,482,240\n","_________________________________________________________________\n"]}],"source":["model.layers[0].trainable = False\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Now we are ready to begin training!\n","\n","\n","Теперь мы готовы приступить к тренировкам!\n","\n","> **Note**: Training full-scale BERT model can be very time consuming! Thus we will only train it for the first 32 batches. This is just to show how model training is set up. If you are interested to try full-scale training - just remove `steps_per_epoch` and `validation_steps` parameters, and prepare to wait!\n","\n","> **Примечание**: Обучение полномасштабной модели BERT может занять очень много времени! Таким образом, мы будем обучать его только для первых 32 партий. Это просто для того, чтобы показать, как настраивается обучение моделей. Если вам интересно попробовать полномасштабное обучение - просто уберите параметры «steps_per_epoch» и «validation_steps» и приготовьтесь ждать!"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.compile('adam','sparse_categorical_crossentropy',['acc'])\n","tf.get_logger().setLevel('ERROR')\n","model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"]},{"cell_type":"markdown","metadata":{},"source":["If you increase the number of iterations and wait long enough, and train for several epochs, you can expect that BERT classification gives us the best accuracy! That is because BERT already understands quite well the structure of the language, and we only need to fine-tune final classifier. However, because BERT is a large model, the whole training process takes a long time, and requires serious computational power! (GPU, and preferably more than one).\n","\n","\n","Если увеличить количество итераций и ждать достаточно долго, и тренироваться в течение нескольких эпох, то можно ожидать, что классификация BERT даст нам наилучшую точность! Это связано с тем, что BERT уже довольно хорошо понимает структуру языка, и нам нужно только точно настроить окончательный классификатор. Однако, поскольку BERT является большой моделью, весь процесс обучения занимает много времени и требует серьезных вычислительных мощностей! (GPU, и желательно более одного).\n","\n","> **Note:** In our example, we have been using one of the smallest pre-trained BERT models. There are larger models that are likely to yield better results.\n","\n","> **Примечание:** В нашем примере мы использовали одну из самых маленьких предварительно обученных моделей BERT. Существуют более крупные модели, которые, вероятно, дадут лучшие результаты."]},{"cell_type":"markdown","metadata":{},"source":["## Takeaway\n","\n","\n","## Еда на вынос\n","\n","In this unit, we have seen very recent model architectures based on **transformers**. We have applied them for our text classification task, but similarly, BERT models can be used for entity extraction, question answering, and other NLP tasks.\n","\n","\n","В этом устройстве мы видели совсем последние модели архитектур на основе **трансформаторов**. Мы применили их для нашей задачи классификации текста, но аналогичным образом модели BERT можно использовать для извлечения сущностей, ответов на вопросы и других задач НЛП.\n","\n","Transformer models represent current state-of-the-art in NLP, and in most of the cases it should be the first solution you start experimenting with when implementing custom NLP solutions. However, understanding basic underlying principles of recurrent neural networks discussed in this module is extremely important if you want to build advanced neural models.\n","\n","Модели трансформаторов представляют собой современное состояние НЛП, и в большинстве случаев это должно быть первое решение, с которым вы начинаете экспериментировать при реализации пользовательских решений НЛП. Однако понимание основных основополагающих принципов рекуррентных нейронных сетей, обсуждаемых в этом модуле, чрезвычайно важно, если вы хотите построить продвинутые нейронные модели."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":4}
