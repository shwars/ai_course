{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Задача классификации текста\n","\n","В этом модуле мы начнем с простой задачи классификации текста на основе набора данных **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)**: наша задача будет состоять в классификации заголовков новостей по одной из 4 категорий: мир, спорт, бизнес и Sci/Tech. \n","\n","## Набор данных\n","\n","Для загрузки набора данных мы будем использовать **[TensorFlow Datasets API](https://www.tensorflow.org/datasets)**."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","\n","dataset = tfds.load('ag_news_subset')"]},{"cell_type":"markdown","metadata":{},"source":["Теперь мы можем получить доступ к обучающей и тестовой частям набора данных, используя `dataset['train']` и `dataset['test']` соответственно:"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train dataset = 120000\n","Length of test dataset = 7600\n"]}],"source":["ds_train = dataset['train']\n","ds_test = dataset['test']\n","\n","print(f\"Length of train dataset = {len(ds_train)}\")\n","print(f\"Length of test dataset = {len(ds_test)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Распечатаем первые 10 заголовков из нашего набора данных: "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n","1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n","2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n","3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n","1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"]}],"source":["classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n","\n","for i,x in zip(range(5),ds_train):\n","    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Векторизация текста\n","\n","Теперь нам нужно преобразовать текст в **векторное представление**, т.е. представить его в виде тензоров, которые можно подать на вход нейросети. Это называется **векторизацией**.\n","\n","Можно использовать два вида представлений: \n","* На уровне символов - на вход нейросети подаются отдельные буквы\n","* На уровне слов - на вход нейросети подаются целиком слова\n","\n","В обоих случаях используется одинаковый подход:\n","\n","* Используется **токенизатор** для разделения текста на **токены** (это могут быть либо отдельные символы и знаки препинания, либо отдельные слова)\n","* Создается **словарь** из этих токенов.\n","\n","Оба этих шага можно выполнить с помощью слоя **TextVectorization**. Сначала создается экземпляр объекта векторизатора, а затем необходимо вызвать метод `adapt`, чтобы просмотреть весь текст и заполнить словарь.\n","\n","### Ограничение размера словаря\n","\n","В примере с набором данных AG News размер словаря довольно большой, более 100 тысяч слов. Вообще говоря, нам не нужны слова, которые редко присутствуют в тексте — они будут содержаться только в нескольких предложениях, и модель не будет эффективно использовать их при обучении. Таким образом, имеет смысл ограничить размер словаря меньшим числом, передав конструктору векторизатора соответствующий аргумент."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["vocab_size = 50000\n","vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n","vectorizer.adapt(ds_train.take(15000).map(lambda x: x['title']+' '+x['description']))"]},{"cell_type":"markdown","metadata":{},"source":["> **Примечание**, что мы используем только подмножество всего набора данных для создания словарного запаса, чтобы ускорить время выполнения. При этом мы рискуем, что некоторые слова из всего датасета не будут включены в словарь - при этом они будут проигнорированы во время обучения. Таким образом, использование всего словарного запаса и прохождение всего набора данных для построения словаря может повысить конечную точность, но скорее всего незначительно.\n","\n","Теперь мы можем получить доступ к фактическому словарю:"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['', '[UNK]', 'the', 'to', 'a', 'of', 'in', 'and', 'on', 'for']\n","Length of vocabulary: 36705\n"]}],"source":["vocab = vectorizer.get_vocabulary()\n","vocab_size = len(vocab)\n","print(vocab[:10])\n","print(f\"Length of vocabulary: {vocab_size}\")"]},{"cell_type":"markdown","metadata":{},"source":["С помощью векторизатора мы можем легко закодировать любой текст в набор чисел:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 290, 1991,    3,  317,   12, 1076, 1811], dtype=int64)>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer('I love to play with my words')"]},{"cell_type":"markdown","metadata":{},"source":["## Мешок слов (Bag-of-Words, BoW)\n","\n","Поскольку многие слова несут на себе некоторое конкретное значение, иногда мы можем выяснить общий смысл фрагмента текста, просто взглянув на отдельные имеющиеся в нем слова, независимо от их порядка в предложении. Например, при классификации новостей такие слова, как *погода* и *снег*, скорее всего, будут указывать *прогноз погоды*, в то время как такие слова, как *акции* и *доллар*, будут засчитываться в *финансовые новости*.\n","\n","Представление **мешок слов** (*Bag-of-words*, BoW) - это наиболее простое векторное представление текста. Мы видели ранее, что каждое слово в тексте связывается с некоторым индексом в словаре. Текст представляется вектором размерностью с количество слов в словаре, и $i$-й элемент содержит количество вхождений слова с индексом $i$ в документ.\n","\n","![Изображение, показывающее, как в памяти представлено векторное представление мешка слов.](images/bag-of-words-example.png) \n","\n","> **Примечание**: Вы также можете думать о BoW как о сумме всех one-hot-encoded векторов индексов отдельных слов в тексте.\n","\n","Ниже приведен пример того, как создать представление мешка слов с помощью библиотеки Scikit Learn:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","sc_vectorizer = CountVectorizer()\n","corpus = [\n","        'I like hot dogs.',\n","        'The dog ran fast.',\n","        'Its hot outside.',\n","    ]\n","sc_vectorizer.fit_transform(corpus)\n","sc_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"]},{"cell_type":"markdown","metadata":{},"source":["Мы также можем использовать векторизатор Keras, определённый нами выше, преобразуя каждое слово в one-hot-encoding и складывая:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def to_bow(text):\n","    return tf.reduce_sum(tf.one_hot(vectorizer(text),vocab_size),axis=0)\n","\n","to_bow('My dog likes hot dogs on a hot day.').numpy()"]},{"cell_type":"markdown","metadata":{},"source":["> **Примечание**: Результат отличается от предыдущего примера, поскольку в случае с векторизатором Keras длина вектора соответствует размеру словаря, который был построен из всего набора данных AG News, в то время как в примере Scikit Learn мы построили словарь из небольшого фрагмента текста на лету. "]},{"cell_type":"markdown","metadata":{},"source":["## Обучение классификатора BoW\n","\n","Теперь, когда мы узнали, как построить представление нашего текста в виде мешка слов, давайте обучим классификатор, который его использует. Во-первых, нам нужно преобразовать наш набор данных в представление мешка слов. Этого можно достичь, используя функцию `map` следующим образом:"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["batch_size = 128\n","\n","ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n","ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["Теперь определим простой нейросетевой классификатор с одним линейным слоем. Входной размер — `vocab_size`, а выходной размер соответствует количеству классов (4). Поскольку мы решаем задачу классификации, конечной функцией активации является **softmax**:"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  1/938 [..............................] - ETA: 11:44 - loss: 1.3802 - acc: 0.2812"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\GIT\\ai_miit\\06-NLP\\02-BowTFIDF\\Notebooks\\TextRepresentation.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m4\u001b[39m,activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m,input_shape\u001b[39m=\u001b[39m(vocab_size,))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train_bow,validation_data\u001b[39m=\u001b[39;49mds_test_bow)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(4,activation='softmax',input_shape=(vocab_size,))\n","])\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.fit(ds_train_bow,validation_data=ds_test_bow)"]},{"cell_type":"markdown","metadata":{},"source":["Поскольку у нас 4 класса, точность выше 80% является хорошим результатом.\n","\n","## Обучение классификатора как одной сети\n","\n","Поскольку векторизатор также является слоем Keras, мы можем определить сеть, которая включает его как первый слой, и обучать такую сеть, подавая ей на вход текст. Таким образом, нам не нужно предварительно векторизировать набор данных с помощью `map`, мы можем просто передать исходный набор данных на вход сети.\n","\n","> **Примечание**: Нам все равно придется применять `map` к нашему набору данных для преобразования полей в исходном датасете (таких как `title`, `description` и `label`) в кортежи. Однако при использовании сторонних данных мы можем сразу построить набор данных с требуемой структурой."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 1)]               0         \n","_________________________________________________________________\n","text_vectorization (TextVect (None, None)              0         \n","_________________________________________________________________\n","tf.one_hot (TFOpLambda)      (None, None, 36705)       0         \n","_________________________________________________________________\n","tf.math.reduce_sum (TFOpLamb (None, 36705)             0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 146824    \n","=================================================================\n","Total params: 146,824\n","Trainable params: 146,824\n","Non-trainable params: 0\n","_________________________________________________________________\n"," 17/938 [..............................] - ETA: 21:42 - loss: 1.3663 - acc: 0.3755"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\GIT\\ai_miit\\06-NLP\\02-BowTFIDF\\Notebooks\\TextRepresentation.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train\u001b[39m.\u001b[39;49mmap(tupelize)\u001b[39m.\u001b[39;49mbatch(batch_size),validation_data\u001b[39m=\u001b[39;49mds_test\u001b[39m.\u001b[39;49mmap(tupelize)\u001b[39m.\u001b[39;49mbatch(batch_size))\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def extract_text(x):\n","    return x['title']+' '+x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x),x['label'])\n","\n","inp = keras.Input(shape=(1,),dtype=tf.string)\n","x = vectorizer(inp)\n","x = tf.reduce_sum(tf.one_hot(x,vocab_size),axis=1)\n","out = keras.layers.Dense(4,activation='softmax')(x)\n","model = keras.models.Model(inp,out)\n","model.summary()\n","\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Биграммы, триграммы и n-граммы\n","\n","Одним из ограничений подхода BoW является то, что некоторые слова являются частью многословных выражений, например, слово hot dog (хот-дог) имеет совершенно иное значение, чем слова hot (горячий) и dog (собака) в других контекстах. Если мы представляем слова «горячий» и «собака» используя для всех контекстов одни и те же векторы, то наша модель BoW не сможет отличать разницу в смыслах.\n","\n","> Более сложные нейросетевые модели не страдают таким недостатком, поскольку, как и свёрточные сети, они могут вычленять паттерны в последовательностях слов. Но в BoW порядок слов вообще никак не учитывается.\n","\n","Чтобы решить эту проблему, совместно с BoW часто используются **биграммы**, т.е. последовательности из двух соседних слов. Для обучения классификторов имеют значения частота каждого слова в отдельности, а также биграмм, триграмм и т.д. В биграммовом представлении мы добавляем в словарь все пары слов, в дополнение к оригинальным словам.\n","\n","Ниже приведен пример того, как создать мешок биграмм с помощью Scikit Learn:"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary:\n"," {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"]},{"data":{"text/plain":["array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","      dtype=int64)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n","corpus = [\n","        'I like hot dogs.',\n","        'The dog ran fast.',\n","        'Its hot outside.',\n","    ]\n","bigram_vectorizer.fit_transform(corpus)\n","print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n","bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"]},{"cell_type":"markdown","metadata":{},"source":["Основным недостатком n-граммового подхода является то, что размер словаря начинает расти чрезвычайно быстро. На практике нам нужно объединить n-граммовое представление с техникой уменьшения размерности, такой как *embeddings*, о которых мы поговорим в следующем блоке.\n","\n","Чтобы использовать n-граммовое представление в нашем наборе данных **AG News**, нам нужно передать параметр `ngrams` конструктору `TextVectorization`. Длина биграммового словаря получается **значительно больше**, в нашем случае это более 1,3 миллиона токенов! Имеет смысл ограничить количество биграмм-токенов каким-то разумным числом.\n","\n","Код выше для обучения классификатора может быть использовать почти без изменений, однако это оказывается очень неэффективным для памяти. В следующем блоке мы будем обучать биграмм-классификатор с помощью эмбеддингов. А пока вы можете поэкспериментировать с обучением биграмм-классификатора в этом блокноте и посмотреть, сможете ли вы получить более высокую точность."]},{"cell_type":"markdown","metadata":{},"source":["## Автоматическое вычисление векторов BoW\n","\n","В приведенном выше примере мы рассчитали векторы BoW вручную, суммируя one-hot-encodings отдельных слов. Однако последняя версия TensorFlow позволяет автоматически вычислять векторы BoW, передавая параметр `output_mode='count'` конструктору векторизатора. Это значительно упрощает определение и обучение нашей модели:"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training vectorizer\n","126/938 [===>..........................] - ETA: 14s - loss: 1.0657 - acc: 0.7289"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\GIT\\ai_miit\\06-NLP\\02-BowTFIDF\\Notebooks\\TextRepresentation.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39madapt(ds_train\u001b[39m.\u001b[39mtake(\u001b[39m500\u001b[39m)\u001b[39m.\u001b[39mmap(extract_text))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/02-BowTFIDF/Notebooks/TextRepresentation.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train\u001b[39m.\u001b[39;49mmap(tupelize)\u001b[39m.\u001b[39;49mbatch(batch_size),validation_data\u001b[39m=\u001b[39;49mds_test\u001b[39m.\u001b[39;49mmap(tupelize)\u001b[39m.\u001b[39;49mbatch(batch_size))\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\winapp\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = keras.models.Sequential([\n","    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='count'),\n","    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n","])\n","print(\"Training vectorizer\")\n","model.layers[0].adapt(ds_train.take(15000).map(extract_text))\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Поскольку наша модель - однослойная, то мы можем интерпретировать значения весов слоя классификатора:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w = model.layers[1].weights[0]\n","w.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Посмотрим на топ-10 самых важных слов для каждой из новостных категорий. Для этого используем `argsort` чтобы получить индексы самых больших весов в строке, и затем возьмём слова из словаря в соответствюущей позиции:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","vocab = model.layers[0].get_vocabulary()\n","\n","pd.DataFrame( { classes[k] : [ vocab[i] for i in np.argsort(w[:,k])[::-1][:20]] for k in range(4) })"]},{"cell_type":"markdown","metadata":{},"source":["## Term frequency - inverse document frequency (TF-IDF)\n","\n","В представлении BoW частота вхождений слов учитывается независимо от самого слова. Однако можно заметить, что частые слова, такие как *a* и *in*, гораздо менее важны для классификации, чем специализированные термины. \n","\n","**TF-IDF** расшифровывается как **term frequency - inverse document frequency**. Это разновидность BoW, где количество вхождений слов в документе нормируется с учётом частоты встречаемости слова в корпусе.\n","\n","Более формально вес $w_{ij}$ слова $i$ в документе $j$ определяется как:\n","$$\n","w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n","$$\n","где\n","* $tf_{ij}$ — количество вхождений $i$ в $j$, т.е. значение BoW, которое мы видели ранее\n","* $N$ - количество документов в коллекции\n","* $df_i$ - количество документов во всей коллекции, содержащих слово $i$\n","\n","Значение TF-IDF $w_{ij}$ увеличивается пропорционально количеству появлений слова в документе и уменьшается пропорционально количеству документов в корпусе, содержащем слово, что помогает скорректировать тот факт, что некоторые слова появляются чаще, чем другие. Например, если слово появляется в *каждом* документе в коллекции, $df_i=N$, и $w_{ij}=0$, т.е. эти термины будут полностью проигнорированы.\n","\n","Вы можете легко создать tf-IDF векторизацию текста с помощью Scikit Learn:"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n","        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ]])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(ngram_range=(1,2))\n","vectorizer.fit_transform(corpus)\n","vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"]},{"cell_type":"markdown","metadata":{},"source":["В Keras слой `TextVectorization` может автоматически вычислять частоты TF-IDF - нам лишь нужно передать параметр `output_mode='tf-idf`: "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training vectorizer\n","938/938 [==============================] - 12s 12ms/step - loss: 0.4197 - acc: 0.8662 - val_loss: 0.3432 - val_acc: 0.8849\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x20c729dfd30>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model = keras.models.Sequential([\n","    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='tf-idf'),\n","    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n","])\n","print(\"Training vectorizer\")\n","model.layers[0].adapt(ds_train.take(500).map(extract_text))\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"]},{"cell_type":"markdown","metadata":{},"source":["## Заключение \n","\n","Несмотря на то, что представление TF-IDF позволяет учитывать действительно важные слова, оно по-прежнему не учитывает порядок слов и, следовательно, контекст. Как сказал в 1935 году известный лингвист Дж.Р. Ферт: «Полное значение слова всегда контекстуально, и изучение значения, не учитывающее контекст, не может восприниматься всерьез». Мы узнаем, как собирать контекстную информацию из текста с помощью языкового моделирования позже в курсе."]}],"metadata":{"kernel_info":{"name":"conda-env-py37_tensorflow-py"},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"nteract":{"version":"nteract-front-end@1.0.0"},"vscode":{"interpreter":{"hash":"d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"}}},"nbformat":4,"nbformat_minor":4}
