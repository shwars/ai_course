{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Генеративные сети\n","\n","Рекуррентные нейронные сети (RNN) и их более сложные разновидности, такие как сети долговременной кратковременной памяти (LSTM) и GRU, дают нам механизм моделирования языка, то есть они могут выучивать порядок слов и предоставлять прогнозы для следующего слова в последовательности. Это позволяет нам использовать RNN для **генеративных задач**, таких как генерация обычного текста, машинный перевод и даже субтитры к изображениям.\n","\n","В архитектуре RNN, которую мы обсуждали в предыдущем разделе, каждый блок RNN производил следующее скрытое состояние в качестве выхода. Тем не менее, мы также можем добавить еще один выход к каждой рекуррентной единице, что позволит нам породить **последовательность** (которая равна по длине исходной последовательности). Более того, мы можем использовать такие блоки RNN, которые не принимают вход на каждом шаге, а просто берут некоторый вектор начального состояния, а затем производят последовательность выходов.\n","\n","В этом примере мы рассмотрим простую генеративную модель. Для простоты используем посимвольную токенизацию и будем генерировать текст буква за буквой. Во время обучения нам нужно взять некоторый текстовый корпус и разбить его на последовательности букв. "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","import numpy as np\n","\n","ds_train, ds_test = tfds.load('ag_news_subset').values()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Создание словаря и токенизация\n","\n","Чтобы построить генеративную сеть на уровне символов, нам нужно разделить текст на отдельные символы, а не на слова. Слой `TextVectorization`, который мы использовали раньше, не может этого сделать, поэтому у нас есть варианты:\n","\n","* Вручную загрузить текст и выполнить токенизацию, как в [этом официальном примере Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)\n","* Использовать класс `Tokenizer` для токенизации на уровне символов.\n","\n","Мы пойдем по второму пути. `Tokenizer` также может использоваться для токенизации по словам, поэтому у нас будет возможность довольно легко переключаться с посимвольного уровня токенизации на уровень слов.\n","\n","Чтобы выполнить токенизацию на уровне символов, нам нужно передать параметр `char_level=True`:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_text(x):\n","    return x['title']+' '+x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x),x['label'])\n","\n","tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n","tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Мы также хотим использовать один специальный токен для обозначения **конца последовательности**, который мы будем называть `<eos>`. Добавим его вручную в словарь:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eos_token = len(tokenizer.word_index)+1\n","tokenizer.word_index['<eos>'] = eos_token\n","\n","vocab_size = eos_token + 1"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Теперь, чтобы закодировать текст в последовательность чисел, мы можем использовать такой код:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.texts_to_sequences(['Hello, world!'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Обучение генеративной RNN для генерации заголовков\n","\n","Способ, которым мы будем обучать RNN генерировать заголовки новостей, заключается в следующем. На каждом шаге мы возьмем один заголовок, который будет подан на вход RNN, и для каждого входного символа мы попросим сеть сгенерировать следующий за ним символ:\n","\n","![Image showing an example RNN generation of the word 'HELLO'.](./images/rnn-generate.png)\n","\n","Для последнего символа нашей последовательности мы попросим сеть сгенерировать `<eos>`.\n","\n","Важное отличие генеративных RNN заключается в том, что мы получаем выходные данные на каждом шаге RNN, а не только от конечной ячейки. Этого можно достичь, указав параметр `return_sequences` ячейке RNN.\n","\n","Таким образом, во время обучения вход сети представляет собой последовательность закодированных символов некоторой длины, а выход - последовательность той же длины, но сдвинутую на один элемент и заканчивающуюся '<eos>'. Минибатч будет состоять из нескольких таких последовательностей, и нам нужно будет использовать **padding** для выравнивания всех последовательностей.\n","\n","Давайте создадим функцию, подготавливающую такой минибатч. Мы будем сначала разбивать датасет на минибатчи с помощью `.batch()`, а затем применять `map` для преобразования. Функция ниже будет сразу преобразовывать целый минибатч:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def title_batch(x):\n","    x = [t.numpy().decode('utf-8') for t in x]\n","    z = tokenizer.texts_to_sequences(x)\n","    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n","    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Вот что мы делаем в этой функции:\n","\n","* Сначала извлекаем текст из строкового тензора. Поскольку строка представлена в байтовом представлении, преобразуем её в unicode с помощью `decode`\n","* `text_to_sequences` преобразует список строк в список целочисленных тензоров\n","* `pad_sequences` дополняет эти тензоры до их максимальной длины\n","* Наконец, мы применяем к символам one-hot encoding - это первый из возвращаемых результатов, входные последовательности\n","* Для получения выходных последовательностей мы убираем первый символ, добавляем в конец токен `<eos>`\n","\n","Тонкость состоит в том, что эта функция является **питонической** (*Pythonic*), т.е. она не может быть автоматически переведена в вычислительный граф Tensorflow. Мы получим ошибки, если попытаемся использовать эту функцию непосредственно в функции `Dataset.map`. Нам нужно обернуть этот вызов с помощью оболочки `py_function`: "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def title_batch_fn(x):\n","    x = x['title']\n","    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n","    return a,b"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> **Примечание**: Различие между преобразованиями данных на стороны Python или Tensorflow может показаться слишком сложным, и вы можете задаться вопросом, почему мы не преобразуем набор данных с помощью стандартных функций Python, прежде чем передать его в `fit`. Хотя это определенно можно сделать, использование `Dataset.map` имеет огромное преимущество, потому что конвейер преобразования данных выполняется с использованием вычислительного графа Tensorflow, который использует преимущества вычислений GPU и сводит к минимуму необходимость передачи данных между CPU / GPU.\n","\n","Теперь мы можем построить нашу генераторную сеть и начать обучение. Она может быть основана на любой из рассмотренных нами RNN-архитектур (простая, LSTM или GRU), но лучше всё-таки использовать LSTM.\n","\n","Поскольку сеть принимает символы в качестве входных данных, а размер словарного запаса довольно мал, нам не нужен слой эмбеддингов, будем подавать one-hot-encoded вход напрямую на вход ячейке LSTM. Выходной слой будет представлять собой классификатор `Dense`, который преобразует выходные данные LSTM в номера токенов в словаре, а точнее в распределение вероятностей появления таких токенов.\n","\n","Кроме того, поскольку мы имеем дело с последовательностями переменной длины, мы можем использовать слой `Masking` для указания той части последовательности, которую нужно игнорировать. Это не очень критично, потому что нас не очень интересует все, что выходит за рамки токена `<eos>`, но мы будем использовать его ради получения некоторого опыта работы с этим слоем. `input_shape` будет `(None, vocab_size)`, где `None` указывает на последовательность переменной длины, а выход будет иметь размер `(None,vocab_size)`,  как вы можете видеть из `summary`:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," masking (Masking)           (None, None, 84)          0         \n","                                                                 \n"," lstm (LSTM)                 (None, None, 128)         109056    \n","                                                                 \n"," dense (Dense)               (None, None, 84)          10836     \n","                                                                 \n","=================================================================\n","Total params: 119,892\n","Trainable params: 119,892\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["2022-12-04 01:51:51.761621: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n","2022-12-04 01:51:52.502525: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n","2022-12-04 01:51:56.147378: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"]},{"name":"stdout","output_type":"stream","text":["15000/15000 [==============================] - 1655s 110ms/step - loss: 1.5458\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x29d8b5bb0>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model = keras.models.Sequential([\n","    keras.layers.Masking(input_shape=(None,vocab_size)),\n","    keras.layers.LSTM(128,return_sequences=True),\n","    keras.layers.Dense(vocab_size,activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(loss='categorical_crossentropy')\n","\n","model.fit(ds_train.batch(8).map(title_batch_fn))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Генерация выходных данных\n","\n","Теперь, когда мы обучили модель, мы можем использовать её для генерации текста. Прежде всего, нам нужен способ декодирования текста, представленного последовательностью номеров токенов. Для этого мы могли бы использовать функцию `tokenizer.sequences_to_texts`; однако она плохо работает с токенизацией на уровне символов. Поэтому мы возьмем словарь токенов из токенизатора (называемый `word_index`), построим обратную карту и напишем собственную функцию декодирования:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n","\n","def decode(x):\n","    return ''.join([reverse_map[t] for t in x])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Теперь давайте займемся генерацией. Мы начнем с некоторой строки `start`, закодируем ее в последовательность `inp`, а затем на каждом шаге будем вызывать нашу сеть, чтобы вывести следующий символ. \n","\n","Выход сети `out` представляет собой вектор элементов размером `vocab_size`, представляющих собой вероятности для каждого токена, и мы можем найти наиболее вероятный номер токена, используя `argmax`. Затем мы добавляем этот символ в сгенерированный список токенов и переходим к следующему шагу генерации. Этот процесс генерации одного символа повторяется `size` раз, чтобы сгенерировать необходимое количество символов, и мы заканчиваем досрочно, когда встречаем `eos`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'Today #39;s to buy to buy to start of the streated in Iraq'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def generate(model,size=100,start='Today '):\n","        inp = tokenizer.texts_to_sequences([start])[0]\n","        chars = inp\n","        for i in range(size):\n","            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n","            nc = tf.argmax(out)\n","            if nc==eos_token:\n","                break\n","            chars.append(nc.numpy())\n","            inp = inp+[nc]\n","        return decode(chars)\n","    \n","generate(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Выборка выходных данных во время обучения \n","\n","Поскольку в случае с генерацией у нас нет никаких простых метрик, таких как *точность*, мы можем наблюдать за тем, как наша модель становится лучше, делая **выборку**, т.е. генерируя некоторую строку во время обучения. Для этого мы будем использовать такую возможность Keras как **обратные вызовы** (*callbacks*), т.е. функции, которые мы передаем функции `fit`, и которые будут периодически вызываться во время обучения. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","  299/15000 [..............................] - ETA: 27:58 - loss: 1.3183"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m sampling_callback \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mLambdaCallback(\n\u001b[1;32m      2\u001b[0m   on_epoch_end \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m batch, logs: \u001b[39mprint\u001b[39m(generate(model))\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train\u001b[39m.\u001b[39;49mbatch(\u001b[39m8\u001b[39;49m)\u001b[39m.\u001b[39;49mmap(title_batch_fn),callbacks\u001b[39m=\u001b[39;49m[sampling_callback],epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["sampling_callback = keras.callbacks.LambdaCallback(\n","  on_epoch_end = lambda batch, logs: print(generate(model))\n",")\n","\n","model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Этот пример уже генерирует довольно хороший текст, но его можно дополнительно улучшить несколькими способами:\n","\n","* **Больше данных**. Мы использовали только заголовки новостей для нашей задачи, но вы можете поэкспериментировать с полным текстом новостей. Помните, что RNN не слишком хороши в обработке длинных последовательностей, поэтому имеет смысл либо разбить их на короткие предложения, либо всегда тренироваться на фиксированной длине последовательности некоторого предопределенного значения `num_chars` (скажем, 256). Вы можете попытаться изменить приведенный выше пример на такую архитектуру, используя [официальный учебник Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/) в качестве вдохновения.\n","\n","* **Многослойный LSTM**. Имеет смысл попробовать 2 или 3 слоя ячеек LSTM. Как мы уже упоминали в предыдущем разделе, каждый слой LSTM извлекает определенные шаблоны из текста, и в случае генератора, работающего на уровне символов, мы можем ожидать, что более низкий уровень LSTM будет отвечать за извлечение слогов, а более высокие уровни - за слова и словосочетания. Это может быть просто реализовано путем передачи параметра `number-of-layer` в конструктор LSTM.\n","\n","* Вы также можете поэкспериментировать с **блоками GRU** и посмотреть, какие из них работают лучше, и с **различными размерами скрытых слоев**. Слишком большой скрытый слой может привести к переобучению (например, сеть будет генерировать на выходе точный текст из входного датасета), а меньший размер может не дать хорошего результата."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Температура генерации\n","\n","В предыдущем определении функции `generate` мы всегда брали символ с наибольшей вероятностью в качестве следующего символа в сгенерированном тексте. Это приводило к тому, что текст часто зацикливался, т.е. циклически перемещался между одними и теми же последовательностями символов снова и снова, как в этом примере:\n","\n","```\n","today of the second the company and a second the company ...\n","```\n","\n","Однако, если мы посмотрим на распределение вероятностей для следующего символа, то может оказаться, что разница между несколькими топовыми вероятностями не так велика, например, один символ может иметь вероятность 0.2, другой - 0.19 и т.д. Например, при поиске следующего символа в последовательности **play**, следующим символом одинаково хорошо может быть либо пробел, либо **e** (как в слове **player**).\n","\n","Это приводит нас к выводу, что не всегда «справедливо» выбирать символ с максимальной вероятностью, потому что выбор второго по величине может в равной степени привести нас к содержательному тексту. Разумнее делать **выборку** символов из словаря в соответствии с распределением вероятностей, полученным на выходе сети.\n","\n","Эта выборка может быть выполнена с помощью функции `np.multinomial`, которая реализует так называемое **мультиномиальное распределение**. Функция, которая реализует такую **мягкую** генерацию текста, определена ниже:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Temperature = 0.3\n","Today #39;s strike #39; to start at the store return\n","On Sunday PO to Be Data Profit Up (Reuters)\n","Moscow, SP wins straight to the Microsoft #39;s control of the space start\n","President olding of the blast start for the strike to pay &lt;b&gt;...&lt;/b&gt;\n","Little red riding hood ficed to the spam countered in European &lt;b&gt;...&lt;/b&gt;\n","\n","--- Temperature = 0.8\n","Today countie strikes ryder missile faces food market blut\n","On Sunday collores lose-toppy of sale of Bullment in &lt;b&gt;...&lt;/b&gt;\n","Moscow, IBM Diffeiting in Afghan Software Hotels (Reuters)\n","President Ol Luster for Profit Peaced Raised (AP)\n","Little red riding hood dace on depart talks #39; bank up\n","\n","--- Temperature = 1.0\n","Today wits House buiting debate fixes #39; supervice stake again\n","On Sunday arling digital poaching In for level\n","Moscow, DS Up 7, Top Proble Protest Caprey Mamarian Strike\n","President teps help of roubler stepted lessabul-Dhalitics (AFP)\n","Little red riding hood signs on cash in Carter-youb\n","\n","--- Temperature = 1.3\n","Today wits flawer ro, pSIA figat's co DroftwavesIs Talo up\n","On Sunday hround elitwing wint EU Powerburlinetien\n","Moscow, Bazz #39;s sentries olymen winnelds' next for Olympite Huc?\n","President lost securitys from power Elections in Smiltrials\n","Little red riding hood vides profit, exponituity, profitmainalist-at said listers\n","\n","--- Temperature = 1.8\n","Today #39;It: He deat: N.KA Asside\n","On Sunday i arry Par aldeup patient Wo stele1\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Temperature = {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_soft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36mgenerate_soft\u001b[0;34m(model, size, start, temperature)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Today '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'On Sunday '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Moscow, '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'President '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Little red riding hood '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["def generate_soft(model,size=100,start='Today ',temperature=1.0):\n","        inp = tokenizer.texts_to_sequences([start])[0]\n","        chars = inp\n","        for i in range(size):\n","            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n","            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n","            probs = probs/np.sum(probs)\n","            nc = np.argmax(np.random.multinomial(1,probs,1))\n","            if nc==eos_token:\n","                break\n","            chars.append(nc)\n","            inp = inp+[nc]\n","        return decode(chars)\n","\n","words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n","    \n","for i in [0.3,0.8,1.0,1.3,1.8]:\n","    print(f\"\\n--- Temperature = {i}\")\n","    for j in range(5):\n","        print(generate_soft(model,size=300,start=words[j],temperature=i))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Мы ввели еще один параметр под названием **температура**, который используется, чтобы указать, насколько сильно мы должны придерживаться наибольшей вероятности. Если температура равна 1.0, мы делаем справедливую мультиномиальную выборку, а когда температура уходит в бесконечность - все вероятности становятся равными, и мы случайным образом выбираем следующий символ. В приведенном ниже примере мы можем наблюдать, что текст становится бессмысленным, когда мы слишком сильно повышаем температуру, и он напоминает «циклический» жестко сгенерированный текст, когда температура становится ближе к 0. "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"}}},"nbformat":4,"nbformat_minor":4}
