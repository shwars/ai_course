{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Распознавание именованных сущностей (NER)\n","\n","В этом примере мы попробуем обучить модель NER на датасете [Entity Annoteted Corpus](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus) из Kaggle. Мы будем использовать файл [ner_dataset.csv](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus?resource=download&select=ner_dataset.csv) - при работе в Colab его необходимо загрузить."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from tensorflow import keras\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["## Подготовка набора данных \n","\n","Загрузим датасет:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\GIT\\ai_miit\\06-NLP\\04-RNNs\\Notebooks\\NER.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../../../data/ner_dataset.zip\u001b[39m\u001b[39m'\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39municode-escape\u001b[39m\u001b[39m'\u001b[39m,compression\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n","\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["df = pd.read_csv('../../../data/ner_dataset.zip',encoding='unicode-escape',compression='zip')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Посмотрим на список уникальных тегов и создадим словари подстановки, которые мы сможем использовать для преобразования тегов в номера классов:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n","       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n","       'I-eve', 'I-nat'], dtype=object)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tags = df.Tag.unique()\n","tags"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'O'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["id2tag = dict(enumerate(tags))\n","tag2id = { v : k for k,v in id2tag.items() }\n","\n","id2tag[0]"]},{"cell_type":"markdown","metadata":{},"source":["Теперь нам нужно сделать то же самое со словарём. Для простоты мы создадим словарный запас без учета частоты слов; в реальной жизни вы можете использовать векторизатор Keras и ограничить количество слов."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["vocab = set(df['Word'].apply(lambda x: x.lower()))\n","id2word = { i+1 : v for i,v in enumerate(vocab) }\n","id2word[0] = '<UNK>'\n","vocab.add('<UNK>')\n","word2id = { v : k for k,v in id2word.items() }"]},{"cell_type":"markdown","metadata":{},"source":["Нам нужно создать набор данных предложений для обучения. Давайте пройдемся по исходному набору данных и разделим все отдельные предложения на `X` (списки слов) и `Y` (список токенов):"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\GIT\\ai_miit\\06-NLP\\04-RNNs\\Notebooks\\NER.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m s,t,\u001b[39mid\u001b[39m \u001b[39m=\u001b[39m [],[],\u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,row \u001b[39min\u001b[39;00m df[[\u001b[39m'\u001b[39m\u001b[39mSentence #\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTag\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m pd\u001b[39m.\u001b[39misna(row[\u001b[39m'\u001b[39m\u001b[39mSentence #\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_miit/06-NLP/04-RNNs/Notebooks/NER.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         s\u001b[39m.\u001b[39mappend(row[\u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["X,Y = [],[]\n","s,t = [],[]\n","for i,row in df[['Sentence #','Word','Tag']].iterrows():\n","    if pd.isna(row['Sentence #']):\n","        s.append(row['Word'])\n","        t.append(row['Tag'])\n","    else:\n","        if len(s)>0:\n","            X.append(s)\n","            Y.append(t)\n","        s,t = [row['Word']],[row['Tag']]\n","X.append(s)\n","Y.append(t)"]},{"cell_type":"markdown","metadata":{},"source":["Теперь векторизуем все слова и токены:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["([29960,\n","  44,\n","  12557,\n","  25236,\n","  22720,\n","  4701,\n","  28455,\n","  15013,\n","  23315,\n","  25845,\n","  8836,\n","  26668,\n","  3866,\n","  18152,\n","  29611,\n","  25845,\n","  17668,\n","  44,\n","  2620,\n","  11835,\n","  6999,\n","  8649,\n","  10764,\n","  10583],\n"," [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def vectorize(seq):\n","    return [word2id[x.lower()] for x in seq]\n","\n","def tagify(seq):\n","    return [tag2id[x] for x in seq]\n","\n","Xv = list(map(vectorize,X))\n","Yv = list(map(tagify,Y))\n","\n","Xv[0], Yv[0]"]},{"cell_type":"markdown","metadata":{},"source":["Для простоты мы дополним все предложения нулевыми токенами до максимальной длины. В реальной жизни мы, скорее всего, захотим использовать masking, и дополнять последовательности до максимальной длины только в рамках одного минибатча."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["X_data = keras.preprocessing.sequence.pad_sequences(Xv,padding='post')\n","Y_data = keras.preprocessing.sequence.pad_sequences(Yv,padding='post')"]},{"cell_type":"markdown","metadata":{},"source":["## NER - это классификация токенов\n","\n","Мы будем использовать двухслойную двунаправленную сеть LSTM для классификации токенов. Чтобы применить финальный полносвязный классификатор к каждому выходу последнего слоя LSTM, мы будем использовать конструкцию `TimeDistributed`, которая реплицирует один и тот же полносвязный слой на каждый из выходов LSTM на каждом шаге: "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 104, 300)          9545400   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 104, 200)          320800    \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 104, 200)          240800    \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, 104, 17)           3417      \n","=================================================================\n","Total params: 10,110,417\n","Trainable params: 10,110,417\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["maxlen = X_data.shape[1]\n","vocab_size = len(vocab)\n","num_tags = len(tags)\n","model = keras.models.Sequential([\n","    keras.layers.Embedding(vocab_size, 300, input_length=maxlen),\n","    keras.layers.Bidirectional(keras.layers.LSTM(units=100, activation='tanh', return_sequences=True)),\n","    keras.layers.Bidirectional(keras.layers.LSTM(units=100, activation='tanh', return_sequences=True)),\n","    keras.layers.TimeDistributed(keras.layers.Dense(num_tags, activation='softmax'))\n","])\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Обратите внимание, что мы явно указываем `maxlen` для нашего набора данных - если мы хотим, чтобы сеть эффективно обрабатывала последовательности переменной длины, нам нужно определять сеть более хитрым способом.\n","\n","Давайте теперь обучим модель. Для демонстрации мы используем только одну эпоху обучения, но вы можете попробовать продолжить обучение в течение более длительного времени. Кроме того, может потребоваться выделить некоторую часть набора данных в качестве тестового датасета, чтобы измерить точность модели."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1499/1499 [==============================] - 866s 573ms/step - loss: 0.0646 - acc: 0.9843\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1f46a366a60>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X_data,Y_data)"]},{"cell_type":"markdown","metadata":{},"source":["## Тестирование результата\n","\n","Давайте теперь посмотрим, как работает наша модель распознавания сущностей на примере предложения: "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["sent = 'John Smith went to Paris to attend a conference in cancer development institute'\n","words = sent.lower().split()\n","v = keras.preprocessing.sequence.pad_sequences([[word2id[x] for x in words]],padding='post',maxlen=maxlen)\n","res = model(v)[0]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["john -> B-per\n","smith -> I-per\n","went -> O\n","to -> O\n","paris -> B-geo\n","to -> O\n","attend -> O\n","a -> O\n","conference -> O\n","in -> O\n","cancer -> O\n","development -> O\n","institute -> O\n"]}],"source":["r = np.argmax(res.numpy(),axis=1)\n","for i,w in zip(r,words):\n","    print(f\"{w} -> {id2tag[i]}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Выводы\n","\n","Даже простая модель LSTM показывает разумные результаты в задачах NER. Однако, чтобы получить более хорошие результаты, вы можете использовать большие предварительно обученные языковые модели, такие как BERT. Обучение BERT для NER с использованием библиотеки Huggingface Transformers описано [здесь](https://huggingface.co/course/chapter7/2)."]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"}}},"nbformat":4,"nbformat_minor":2}
