{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification\n",
    "\n",
    "Classification problem is when you have a dataset of objects belonging to a number of classes, and you need to build a model that will determine the class based on input features. Another way to put it: in regression you need to predict numberic variable, and in classiciation - categorical (mostly nominal). \n",
    "\n",
    "We distinguish between **binary** and **multiclass** classification, the former being a special case of the latter. However, any multi-class classification can be reduced to binary using one-vs-all approach, by building a set of binary classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, let's build an artificial dataset with 2 input features for binary classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_classification(\n",
    "    n_samples=1000,n_features=2,random_state=13,\n",
    "    n_informative=2,n_redundant=0,\n",
    "    class_sep=2,n_clusters_per_class=1,flip_y=0)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Simplest binary classification algorithm is called **logistic regression**. It uses linear function to distinguish between classes $f_{W,b}(x)=Wx+b$, where $W$ - is weights matrix, $b$ - bias.\n",
    "\n",
    "> **Important:** despite the name, logistic regression is a classification algorithm!\n",
    "\n",
    "We can use sign of $f$ to decide which class of the object is given by the model. Since in our dataset $y_i\\in\\{0,1\\}$, predicted value $y={1\\over2}(\\mathrm{sign} f(x)+1)$. We say that an **activation function** $\\sigma(x)={1\\over2}(\\mathrm{sign}(x)+1)$, is used after linear function, i.e. $y=\\sigma(f(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx=np.linspace(-5,5)\n",
    "plt.plot(nx,0.5*(np.sign(nx)+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sign function is not smooth, and in order to use gradient descent optimization we need our function to be continuous and differentiable. Thus, most often **sigmoid** is used as an activation function $\\sigma(x)={1\\over1+e^{-x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5,5)\n",
    "plt.plot(x,sigmoid(x))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "def mse_loss(W,b):\n",
    "  return np.average(np.square(sigmoid(np.matmul(X,W)+b)-Y))\n",
    "\n",
    "mse_loss(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to compute the derivative for gradient descent numerically. Normally we do not do that, because it is computationally expensive, especially in the multi-dimensional case. But let's do it just for demo purposes, and define a function that will partially differentiate a given function `f`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT\\ai_course\\en\\2-ml\\3-Classification.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_course/en/2-ml/3-Classification.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         r[i] \u001b[39m=\u001b[39m (f(x\u001b[39m+\u001b[39mdxc)\u001b[39m-\u001b[39mf(x))\u001b[39m/\u001b[39mdx\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT/ai_course/en/2-ml/3-Classification.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m r\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT/ai_course/en/2-ml/3-Classification.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m d(\u001b[39mlambda\u001b[39;00m x: mse_loss(x,b),W)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "def d(f,x,dx=0.1):\n",
    "    n = x.shape[0]\n",
    "    r = np.zeros_like(x)\n",
    "    for i in range(n):\n",
    "        dxc = np.eye(n)[i]*dx\n",
    "        r[i] = (f(x+dxc)-f(x))/dx\n",
    "    return r\n",
    "\n",
    "d(lambda x: mse_loss(x,b),W) # derivative w.r.t. dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization loop looks the same as for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "eta = 0.1\n",
    "n = len(X)\n",
    "for i in range(100):\n",
    "    print(f\"({W} {b}) -> loss={mse_loss(W,b)}\")\n",
    "    #print(f\"dldw={dldw}, dldb={dldb}, W={W}, b={b}\")\n",
    "\n",
    "    dldw = d(lambda x: mse_loss(x,b),W)\n",
    "    dldb = d(lambda x: mse_loss(W,x),b)\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    W-=eta*dldw\n",
    "    b-=eta*dldb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the boundary between classes looks. Since we determine classes by the sign of $f(x)$, the boundary will be given by the equation $w_0x_0+w_1x_1+b = 0$, where $x_0,x_1$ - our input variables, $w_0, w_1, b$ - model parameters. Thus we can write the equation for the straight line by expressing $x_1$ w.r.t. $x_0$:\n",
    "$$\n",
    "x_1 = {-b\\over w_1}- {w_0\\over w_1}x_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run optimization several times you will see that convegence to the ideal classification boundary is not always good. This is because MSE loss function is not very suited for classifiction, since it will always be in 0-1 interval. In other words, sigmoid \"hides\" the information on how bad our original error was - because it converges to 0 or 1 asymptotically when the output of linear function becomes too big or too small. \n",
    "\n",
    "To compensate for that, we can use logarithmic loss function. For example, if the expected class is $\\hat y=1$, and our model gives the value $y=0.1$ after the sigmoid function, the loss would be $-\\log 0.1\\approx 2.3$. And if predicted value of $y$ goes closer to 0, logarithm would increase even faster! However, if we guessed the class correctly and predicted 1, our loss would be 0.\n",
    "\n",
    "> Output of the model (after sigmoid) can be thought of as a **probability** that the predicted class is 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = np.linspace(0.1,5)/5\n",
    "plt.plot(nx,-np.log(nx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the expected value of $\\hat y=0$ the loss would be $-\\log(1-y)$, where $y$ is our model output. So, if we denote the expected value as $\\hat y \\in\\{0,1\\}$, our logistic loss function would be\n",
    "$$\n",
    "\\mathrm{logloss}(y,\\hat y) = \\hat y\\log y + (1-\\hat y)\\log(1-y)\n",
    "$$\n",
    "\n",
    "> For more general case, if we have $n>2$ classes, our expected class is $\\hat y=i$, and the probability for the $i$-th class predicted by the model is $p_i$ ($\\sum_k p_k=1$), the logistic loss would be given by $-\\log p_{i}$\n",
    "\n",
    "Let's try our optimization loop with logistic loss function. In this example, we come back to using pre-computed derivatives for $W$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "def loss(W, b):\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    return (-Y * np.log(h) - (1 - Y) * np.log(1 - h)).mean()\n",
    "\n",
    "eta = 0.2\n",
    "n = len(X)\n",
    "for i in range(100):\n",
    "    print(f\"({W} {b}) -> loss={loss(W,b)}\")\n",
    "    #print(f\"dldw={dldw}, dldb={dldb}, W={W}, b={b}\")\n",
    "\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    dldw = np.matmul(X.T,h-Y.T)/n\n",
    "    dldb = (h-Y.T).mean()\n",
    "    W-=eta*dldw\n",
    "    b-=eta*dldb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in SkLearn\n",
    "\n",
    "In most of the practical cases, we will use Scikit Learn to train classification models. Let's split our data into train/test dataset and see the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy={sk.metrics.accuracy_score(Y_test,Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict probabilities for individual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the boundary line looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "b = model.intercept_\n",
    "W = model.coef_[0]\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's come back to real-life classification problem using Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to vectorize categorical data. Let's start with sex. It is nominal, but since there are only two values, it does not really matter whether we treat it as nominal or ordinal, i.e. if we use Label Encoding or One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"nsex\"] = df['sex'].apply(lambda x: 0 if x==\"male\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that some values are missing for `age` column. Let's see how many:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several possible options for handling missing values:\n",
    "* Remove all rows with missing values. It is not the ideal solution, because in our case we have more than 10% such lines.\n",
    "* Remove the whole `age` column. Also not a good solution, because it has important predictive power.\n",
    "* Fill all missing values with some value, eg. mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fage'] = df['age'].fillna(df['age'].mean())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our dataset, we can do the train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pclass','nsex','fage','sibsp','parch']\n",
    "X = df[features]\n",
    "Y = df['survived']\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy={sk.metrics.accuracy_score(Y_test,Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand in more detail how the error looks like, we can look at **confusion matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.ConfusionMatrixDisplay.from_estimator(model,X_test,Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we can also measure other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sk.metrics.classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at ROC-curve and AUC value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sk.metrics.roc_curve(Y_test, model.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's try to interpret our model by looking at coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(['pclass','nsex','fage','sibsp','parch'],model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Unlike linear models, decision trees work best for categorical value. A simples algorithm to build a decision tree is called [ID3](https://ru.wikipedia.org/wiki/ID3_(%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC).\n",
    "\n",
    "Let's consider only categorical attribues of our model, eg `sex` and `pclass`. Let's also convert `age` to categorical attribute by **binning**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agecat'] = df['age'].apply(lambda x: \"young\" if x<18 else \"old\" if x>45 else \"middle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to use only `sex` for classification. Let's compute probabilities for different genders separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df['sex'].unique():\n",
    "    print(f\"Probability for sex={s} is {df[df['sex']==s]['survived'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can predict survived=1 for all women, and survived=0 for all men and that would give us certain accuracy.\n",
    "\n",
    "However, what if looking at `pclass` would give us better predictive power? Let's try out all features and see, using which has better predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_by_attr(df,attr):\n",
    "    p = {}\n",
    "    for k in df[attr].unique():\n",
    "        p[k] = int(df[df[attr]==k]['survived'].mean()>=0.5)\n",
    "    return df.apply(lambda x: p[x[attr]]==x['survived'],axis=1).mean()\n",
    "\n",
    "for a in ['pclass','sex','agecat']:\n",
    "    print(f\"Accuracy by {a} = {accuracy_by_attr(df,a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looking at `sex` is the most effective. After that, we can split the dataset into two parts, and for each of them repeat the process. Here's how accuracy will look like if we then classify by `pclass`, for men and women separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex in ['male','female']:\n",
    "    df_1 = df[df['sex']==sex]\n",
    "    print(f\"{sex}, accuracy by pclass = {accuracy_by_attr(df_1,'pclass')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 works with categorical attributes only. There is also more advanced version of the algorithm called [ะก4.5](https://ru.wikipedia.org/wiki/C4.5), developed by the same author. It supports branch pruning, using numerical attributed (i.e. automatic boundary selection in the same manner, as we did with age), and also working with missing values.\n",
    "\n",
    "Let's use decision tree algorithm from SkLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree \n",
    "\n",
    "model = sk.tree.DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "sk.metrics.accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy more or less corresponds to linear model. Depending on the problem, however, we can sometimes get very different results - that is why Automatic ML approach that we have seen before is used to try our different models for any given dataset.\n",
    "\n",
    "Tree models are good, because they are interpretable. Let's try to train a shallow tree and see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.tree.DecisionTreeClassifier(max_depth=3).fit(X_train,Y_train)\n",
    "\n",
    "import graphviz\n",
    "graphviz.backend.dot_command.DOT_BINARY = 'c:/winapp/conda/Library/bin/graphviz/dot.exe'\n",
    "\n",
    "graphviz.Source(\n",
    "    sk.tree.export_graphviz(model,feature_names=features,class_names=['no','yes'],\n",
    "    filled=True, rounded=True, special_characters=True,impurity=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as in our investigation above, `sex` is most important factor at the root of the tree, and then more complex logic is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "Let's talk about one more linear classification model - support vector machine, or SVM. Consider again the 2D case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_classification(\n",
    "    n_samples=1000,n_features=2,random_state=3,\n",
    "    n_informative=2,n_redundant=0,\n",
    "    class_sep=2,n_clusters_per_class=1,flip_y=0)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of SVM is to build such an ideal separation line $w_0x_0+w_1x_1+b=0$, that yields the widest **separation margin** between two classes. By varying the angle of the separation line we can vary the margin. Elements through which lines parallel to the separation line passes are called **support vectors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.svm, sklearn.inspection\n",
    "\n",
    "model = sk.svm.SVC(kernel=\"linear\", C=1000)\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "# build the separaton line\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "# highlight the support vectors\n",
    "ax.scatter(\n",
    "    model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "    s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is in a way similar to logistic regression, but uses different loss function that maximizes the margin. Since we are mostly using support vectors and now all dataset elements to build our separation line (or hyperplane for higher-dimensional cases), the method turns our to be more efficient for large datasets.\n",
    "\n",
    "To separate classes that are not linearly separable, SVM allows using so-called **kernel trick**, by using non-linear **kernel functions**. For example, if we use SVM from Scikit Learn with default parameters, it will use non-linear kernel functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm, sklearn.inspection\n",
    "\n",
    "model = sk.svm.SVC()\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "# showing separation lines\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "# highlight support vectors\n",
    "ax.scatter(\n",
    "    model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "    s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a set of two classes that are not linearly separable. We can use `make_circles` function to generate those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_circles(n_samples=100,factor=0.5,noise=0.1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply SVM with non-linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.svm.SVC()\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accuracy changes when using linear and non-linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "linmodel = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "print(f\"Linear accuracy = {sk.metrics.accuracy_score(Y_test,linmodel.predict(X_test))}\")\n",
    "print(f\"Non-lin SVM accuracy = {sk.metrics.accuracy_score(Y_test,model.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not that we can achieve high accuracy by using linear model and adding extra computed features to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.hstack([X_train,X_train*X_train])\n",
    "X_test_aug = np.hstack([X_test,X_test*X_test])\n",
    "\n",
    "linaugmodel = sk.linear_model.LogisticRegression().fit(X_train_aug,Y_train)\n",
    "\n",
    "print(f\"Augmented linear accuracy = {sk.metrics.accuracy_score(Y_test,linaugmodel.predict(X_test_aug))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same result by automatically generating higher-order features using `PolynomialFeatures` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "\n",
    "pipe = sk.pipeline.Pipeline([\n",
    "    ('PolyTransformer',sk.preprocessing.PolynomialFeatures()),\n",
    "    ('LinearModel',sk.linear_model.LogisticRegression())\n",
    "])\n",
    "pipe.fit(X_train,Y_train)\n",
    "print(f\"Pipeline accuracy = {sk.metrics.accuracy_score(Y_test,pipe.predict(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classifiction\n",
    "\n",
    " As an example of multi-class classification problem, let's consider handwritten digit recognition using [MNIST](https://ru.wikipedia.org/wiki/MNIST_(%D0%B1%D0%B0%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)) dataset - a dataset of handwritten digits collected by US National Institute of Standards. Training dataset contains 60000 digits collected from 250 different students and employees of NIST, and the test dataset of 10000 digits, collected from different individuals. Each digit is represented by 28x28 pixel array in 256 grayscale levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(X.iloc[i].to_numpy().reshape(28,28))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(Y[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole dataset contains 70000 digits. Let's split it into training and test dataset. To speed things up, we will use only 10000 digits for both training and test. You can try to use the whole dataset, but be prepared to wait longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = \\\n",
    "  sk.model_selection.train_test_split(X,Y,train_size=10000, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification algorithms that we have discussed above were **binary**. For multi-class we can use **one-vs-all** approach. Let's train a classifier to classify between 0 and non-0 digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_train = Y_train==\"0\"\n",
    "Y0_test = Y_test==0\n",
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y0_train)\n",
    "sk.metrics.accuracy_score(Y0_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may see that algorithm does not converge. It can be for several reasons:\n",
    "\n",
    "* An optimized used by default in `LogisticRegression` is not expactly gradient descent, but `lbfgs`, which works well on relatively small datasets. You can try specifying different optimizer using `solver=...`.\n",
    "* There is a convention that all input features are scaled to 0..1 interval (or something close to 0..1 or -1..1), and many libraries expect this. It is especially important for data where features have different scales (eg. person's height can vary fro 30 to 250 cm, and age - from 0 to 120), and it is important to scale all input values to the same interval to ensure *fair* training. Before we did not care too much about scaling 0..1, because we did not come across high dimensional problems. Our MNIST model has 768 inputs, and each of them can have values in 0..255 range, so adding all those inputs together can lead to pretty large numbers.\n",
    "\n",
    "Let's normalize all values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg').fit(X_train/255.0,Y0_train)\n",
    "sk.metrics.accuracy_score(Y0_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get nearly 90% accuracy for \"0 vs all\" classifier. For multiclass classification, we need to build 10 such models, one for each digit. Sklearn contains special class that allows us to convert any binary classifier to multi-class classifier automatically using **One vs. All** approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.multiclass\n",
    "\n",
    "model = sk.multiclass.OneVsRestClassifier(\n",
    "    sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1))\n",
    "model.fit(X_train/255.0, Y_train)\n",
    "sk.metrics.accuracy_score(Y_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.ConfusionMatrixDisplay.from_estimator(model,X_test/255.0,Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OneVsRestClassifier` can convert any binary classifier to multiclass. However, many classfiers in SkLearn, including logistic regression, already support multi-class mode. Thus we can feed multi-class data directly into `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1)\n",
    "model.fit(X_train/255.0, Y_train)\n",
    "sk.metrics.accuracy_score(Y_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to look at the coefficients of the model. In multi-class case, the shape of `model.coef_` would be $10\\times784$ - it corresponds to 10 classes, each having $768=28\\times28$ coefficients. We can try and visualize it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,10,figsize=(15,6))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(model.coef_[i].reshape(28,28))\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture explains how the classifier works - it multiplies the pattern from coefficient vector by the input image, and the output is higher when the pattern corresponds to the input image. Positive coefficients in the weight pattern correspond to those pixels that should be present for the digit being recognized, and negative - to the pixels that suggest that the digit is not the intended one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "To test the accuracy of the model we have done train-test split. However, in this case one part of the dataset is \"lost\" and not used for training. Also, there is always a chance that this particular split is not ideal, and yields the results that are somehow higher or lower that the average.\n",
    "\n",
    "To avoid thos problems, we can use **cross-validation** (*K-Fold Cross-Validation*). In this case, our dataset is split into $k$ parts, and $k$ training experiments are performed. During each experiment, one of the parts is used for validation (each time - different part), and the rest for training. Obtained metric results are then averaged out.\n",
    "\n",
    "Scikit Learn can train any model using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92192857, 0.92128571, 0.91521429, 0.91828571, 0.92978571])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1)\n",
    "scores = sk.model_selection.cross_val_score(model,X/255.0,Y,cv=5) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9212999999999999, 0.004868767601681531)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cross_val_score` method we can speficy different metrics to be estimated, and also different strategies for splitting the dataset (using `cv` parameter).\n",
    "\n",
    "There is also a method called `cross_val_predict`, which returns the predicted value for each element in the dataset, predicted by the model which as not been trained on that element.\n",
    "\n",
    "If we need to returned actual trained models during cross-validation, we need to use `cross_validate` method. More about it can be found [in documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
