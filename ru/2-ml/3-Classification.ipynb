{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Машинное обучение: задача классификации\n",
    "\n",
    "Задача классификации ставится, когда необходимо разделить объекты в датасете на два или более классов. Можно сказать, что в рассмотренной нами задаче регрессии целевая переменная - числовая, а в задаче классификации - категориальная.\n",
    "\n",
    "Обычно отдельно выделяют **бинарную** или **двоичную классификацию**, и **многоклассовую** (которая является более общим случаем бинарной). Многоклассовая классификация может быть сведена к двоичной путём построения нескольких классификаторов one-vs-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посмотреть, как решается задача классификации, сгенерируем случайный датасет. Для наглядности будем использовать размерность задачи 2, чтобы можно было визуализировать картину на плоскости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_classification(\n",
    "    n_samples=1000,n_features=2,random_state=13,\n",
    "    n_informative=2,n_redundant=0,\n",
    "    class_sep=2,n_clusters_per_class=1,flip_y=0)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия\n",
    "\n",
    "Простейший линейный алгоритм решения задачи двоичной классификации - это **логистическая регрессия**. В ней используется линейная решающая функция $f_{W,b}(x)=Wx+b$, где $W$ - матрица весов (weights), $b$ - сдвиг (bias).\n",
    "\n",
    "> **Замечание:** несмотря на название, логистическая регрессия решает задачу классификации!\n",
    "\n",
    "Можно считать, что класс объекта определяется знаком решающей функции $f$. Поскольку в нашем датасете $y_i\\in\\{0,1\\}$, предсказываемое значение $y={1\\over2}(\\mathrm{sign} f(x)+1)$. Говорят, что после линейного преобразования применяется **передаточная функция** $\\sigma(x)={1\\over2}(\\mathrm{sign}(x)+1)$, а $y=\\sigma(f(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx=np.linspace(-5,5)\n",
    "plt.plot(nx,0.5*(np.sign(nx)+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако данная передаточная функция не являтеся гладкой, а для использования метода градиентного спуска нам хотелось бы иметь дифференцируемую функцию. Поэтому обычно в качестве передаточно функции используют функцию **сигмоид** $\\sigma(x)={1\\over1+e^{-x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5,5)\n",
    "plt.plot(x,sigmoid(x))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем, как и в прошлый раз, метод градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "def mse_loss(W,b):\n",
    "  return np.average(np.square(sigmoid(np.matmul(X,W)+b)-Y))\n",
    "\n",
    "mse_loss(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производную в процессе градиентного спуска можно считать и численно. Так обычно не делают, поскольку для многомерного случая это слишком вычислительно сложный процесс, но для примера - попробуем так сделать. Опишем функцию численного дифференцирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(f,x,dx=0.1):\n",
    "    n = x.shape[0]\n",
    "    r = np.zeros_like(x)\n",
    "    for i in range(n):\n",
    "        dxc = np.eye(n)[i]*dx\n",
    "        r[i] = (f(x+dxc)-f(x))/dx\n",
    "    return r\n",
    "\n",
    "d(lambda x: mse_loss(x,b),W) # производная по dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл оптимизации выглядит также, как и в случае регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "eta = 0.1\n",
    "n = len(X)\n",
    "for i in range(100):\n",
    "    print(f\"({W} {b}) -> loss={mse_loss(W,b)}\")\n",
    "    #print(f\"dldw={dldw}, dldb={dldb}, W={W}, b={b}\")\n",
    "\n",
    "    dldw = d(lambda x: mse_loss(x,b),W)\n",
    "    dldb = d(lambda x: mse_loss(W,x),b)\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    W-=eta*dldw\n",
    "    b-=eta*dldb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как разделяющая прямая выглядит на графике. Уравнение разделяющей прямой имеет вид $w_0x+w_1y+b = 0$, где $x,y$ - переменные по осям абсцисс и ординат на графике, $w_0, w_1, b$ - веса модели. Отсюда получаем уравение прямой, выражая $y$ через $x$:\n",
    "$$\n",
    "y = {-b\\over w_1}- {w_0\\over w_1}x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что мы получили неплохую разделяющую прямую, но если мы попробуем запустить процесс несколько раз то увидим, что он не всегда хорошо сходится. Это происходит потому, что функция потерь MSE для задач классификации находится в пределах 0-1, поскольку худщее, что может случиться - это мы неправильно угадаем класс. Иными словами, сигмоид как передаточная функция теряет информацию о том, насколько сильно мы ошиблись - из-за того, что он асимптотически быстро стремиться к 0 или к 1 при больших по модулю значениях линейной функции.\n",
    "\n",
    "Для того, чтобы компенсировать это, в качестве функции ошибки для задач регрессии используют логарифмическую функцию потерь. Например, если ожидаемый класс $\\hat y=1$, а на выходе нашей модели (после сигмоиды) мы получаем значение $y=0.1$, то ошибка будет равна $-\\log 0.1\\approx 2.3$. А при стремлении решающей функции к 0 лорагифм вообще будет неограниченно возрастать! Если же мы угадали с классом и предскзали значение 1, то ошибка будет равна нулю.\n",
    "\n",
    "> Выход решающей функции (после сигмоиды) можно рассматривать как **вероятность** того, что ожидаемый класс - 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = np.linspace(0.1,5)/5\n",
    "plt.plot(nx,-np.log(nx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Аналогично, для ожидаемого класса $\\hat y=0$ ошибка будет равна $-\\log(1-y)$, где $y$ - выход решающей функции. С учётом этого, если обозначить ожидаемый класс как $\\hat y \\in\\{0,1\\}$, то функция ошибки будет равна\n",
    "$$\n",
    "\\mathrm{logloss}(y,\\hat y) = \\hat y\\log y + (1-\\hat y)\\log(1-y)\n",
    "$$\n",
    "\n",
    "> Для более общего случая, когда мы имеем $n>2$ классов, вероятность появления $i$-го класса предсказана как $p_i$ ($\\sum p_i=1$), при этом номер правильного класса - $\\hat y$, логистическая функция ошибка равна $-\\log p_{\\hat y}$. \n",
    "\n",
    "$$\n",
    "-\\log p_{\\hat y}\n",
    "$$\n",
    "\n",
    "Пробуем запустить оптимизацию с логистической функцией ошибки. В этом примере мы используем явно вычисленные производные ошибки по $W$ и $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(2,))\n",
    "b = np.random.normal(size=(1,))\n",
    "\n",
    "def loss(W, b):\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    return (-Y * np.log(h) - (1 - Y) * np.log(1 - h)).mean()\n",
    "\n",
    "eta = 0.2\n",
    "n = len(X)\n",
    "for i in range(100):\n",
    "    print(f\"({W} {b}) -> loss={loss(W,b)}\")\n",
    "    #print(f\"dldw={dldw}, dldb={dldb}, W={W}, b={b}\")\n",
    "\n",
    "    h = sigmoid(np.matmul(X,W)+b)\n",
    "    dldw = np.matmul(X.T,h-Y.T)/n\n",
    "    dldb = (h-Y.T).mean()\n",
    "    W-=eta*dldw\n",
    "    b-=eta*dldb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация в SkLearn\n",
    "\n",
    "Мы можем использовать библиотеку Scikit Learn для решения задач классификации. Разобьем на обучающую и проверочную выборки, и посмотрим на точность модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy={sk.metrics.accuracy_score(Y_test,Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также предсказывать вероятности принадлежности классам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит разделяющая прямая:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "xs = X[:,0].min(), X[:,0].max()\n",
    "b = model.intercept_\n",
    "W = model.coef_[0]\n",
    "plt.plot(xs,[-b/W[1]-x*W[0]/W[1] for x in xs])\n",
    "plt.ylim([-2,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим практическую задачу - классификацияю пассажиров Титаника. Для начала загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../data/titanic.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нам необходимо проделать преобразование категориальных признаков в числовые, как мы рассматривали в прошлой лекции. В нашем случае, необходимо преобразовать пол, который является номинальным признаком. Поскольку значения всего два, не очень принципиально, использовать ли Label Encoding или One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"nsex\"] = df['sex'].apply(lambda x: 0 if x==\"male\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно заметить, что некоторые значения возраста отсутствуют. Посмотрим, сколько таких строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть несколько вариантов, как быть с этим показателем:\n",
    "* Убрать все строки, где есть отсутствующие значения. Это слишком дорогой способ, поскольку у нас значения отстуствуют в более, чем 10% строк.\n",
    "* Убрать весь столбец `age` из рассмотрения. Также не очень хороший вариант\n",
    "* Заполнить отсутствующие значения некоторыми другими, например, средним:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fage'] = df['age'].fillna(df['age'].mean())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы очистили данные, можем извлечь интересующие нас признаки и разбить на обучающую и тестовую выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pclass','nsex','fage','sibsp','parch']\n",
    "X = df[features]\n",
    "Y = df['survived']\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy={sk.metrics.accuracy_score(Y_test,Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы поподробнее разобраться в том, как устроена ошибка, посмотрим на так называемую **матрицу ошибок** (*confusion matrix*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.ConfusionMatrixDisplay.from_estimator(model,X_test,Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации имеет смысл говорить также про другие метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sk.metrics.classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит ROC-кривая для нашей модели и метрика AUC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sk.metrics.roc_curve(Y_test, model.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем интерпретировать линейную модель, посмотрев на коэффициенты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(['pclass','nsex','fage','sibsp','parch'],model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Деревья решений\n",
    "\n",
    "В отличие от линейных моделей, деревья решений хорошо работают для категориальных атрибутов. Простейший алгоритм построения дерева называется [ID3](https://ru.wikipedia.org/wiki/ID3_(%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC).\n",
    "\n",
    "Будем рассматривать только категориальные атрибуты нашей модели - пол (`sex`) и класс (`pclass`). Добавим для интереса ещё один категориальный атрибут: возраст:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agecat'] = df['age'].apply(lambda x: \"young\" if x<18 else \"old\" if x>45 else \"middle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, мы хотим предсказывать вероятность выживания только на основе пола. В этом случае, средняя вероятность равна:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df['sex'].unique():\n",
    "    print(f\"Probability for sex={s} is {df[df['sex']==s]['survived'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.е. мы можем предсказывать \"выжил\" для всех женщин, и \"не выжил\" для все мужчин - и это будет давать нам некоторую точность предсказания.\n",
    "\n",
    "Но вдруг было бы правильнее сначала смотреть не на пол, а на класс? Посмотрим, какая точность была бы у результирующей модели, если бы мы выбрали в качестве решающего атрибута пол, класс или возрастную категорию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_by_attr(df,attr):\n",
    "    p = {}\n",
    "    for k in df[attr].unique():\n",
    "        p[k] = int(df[df[attr]==k]['survived'].mean()>=0.5)\n",
    "    return df.apply(lambda x: p[x[attr]]==x['survived'],axis=1).mean()\n",
    "\n",
    "for a in ['pclass','sex','agecat']:\n",
    "    print(f\"Accuracy by {a} = {accuracy_by_attr(df,a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, выгоднее всего сначала смотреть на пол пассажира. Далее, разделяем датасет на два подмножества, и для каждого из них повторяем этот процесс. Например, вот как будет выглядеть точность для мужчин и женщин, если смотреть далее на атрибут `pclass`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex in ['male','female']:\n",
    "    df_1 = df[df['sex']==sex]\n",
    "    print(f\"{sex}, accuracy by pclass = {accuracy_by_attr(df_1,'pclass')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усовершенствованным вариантов ID3 является алгоритм [С4.5](https://ru.wikipedia.org/wiki/C4.5), разработанный тем же автором, Джоном Квинланом. В нём поддерживается отсечение ветвей, работа с числовыми атрибутами (т.е. автоматическое добавление оптимального разделения наподобие того, как мы вручную делали с возрастом), а также работа с неполной выборкой (с отсутствующими атрибутами).\n",
    "\n",
    "Применим классификацию в виде деревья решений из SkLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree \n",
    "\n",
    "model = sk.tree.DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "sk.metrics.accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, точность примерно соответствует линейной модели. В зависимости от задачи, однако, мы можем получать разную точность - именно поэтому имеет право на жизнь подход Automatic ML, о котором мы говорили ранее.\n",
    "\n",
    "Плюсом деревьев решений является их интерпретируемость. Можем попробовать построить дерево небольшой глубины, и затем его визуализировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.tree.DecisionTreeClassifier(max_depth=3).fit(X_train,Y_train)\n",
    "\n",
    "import graphviz\n",
    "graphviz.backend.dot_command.DOT_BINARY = 'c:/winapp/conda/Library/bin/graphviz/dot.exe'\n",
    "\n",
    "graphviz.Source(\n",
    "    sk.tree.export_graphviz(model,feature_names=features,class_names=['no','yes'],\n",
    "    filled=True, rounded=True, special_characters=True,impurity=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что как и в нашем случае пол является главным решающим фактором, после чего наступают более сложные решения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Машина опорных векторов (SVM)\n",
    "\n",
    "В заключение поговорим о ещё одной линейной модели - машине опорных векторов. Для этого вернёмся к исходной задаче - разделении классов точек на плоскости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_classification(\n",
    "    n_samples=1000,n_features=2,random_state=3,\n",
    "    n_informative=2,n_redundant=0,\n",
    "    class_sep=2,n_clusters_per_class=1,flip_y=0)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея метода опорных векторов состоит в том, чтобы построить такую разделяющую прямую $w_0x_0+w_1x_1+b=0$, которая бы обеспечивала максимальный \"зазор\" между разделяемыми классами. Варьируя угол наклона разделяющей прямой мы добиваемся максимального зазора. При этом элементы, на которые \"опираются\" прямые, называются **опорными векторами**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.svm, sklearn.inspection\n",
    "\n",
    "model = sk.svm.SVC(kernel=\"linear\", C=1000)\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "# строим разделяющие прямые\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "# обводим опорные вектора\n",
    "ax.scatter(\n",
    "    model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "    s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный метод похож на логистическую регрессию, но использует другую функцию ошибки, позволяющую уменьшить зазор. За счет того, что при построении разделяющей прямой (или гиперплоскости, в случае большей размерности) используются близлежащие точки - опорные вектора, повышается эффективность метода для большого количества входных обучающих данных.\n",
    "\n",
    "Для разделения линейно - неразделимых множеств SVM допускает использование нелинейных **функций ядра** (*kernel functions*). Например, если мы будет использовать метод SVM из Scikit Learn по умолчанию, то будут использованы нелинейные функции ядра: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm, sklearn.inspection\n",
    "\n",
    "model = sk.svm.SVC()\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "# строим разделяющие прямые\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "# обводим опорные вектора\n",
    "ax.scatter(\n",
    "    model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "    s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим случай, когда множества точек не являются линейно разделимыми. Такой датасет мы можем сгненерировать при помощи функции `make_circles`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk.datasets.make_circles(n_samples=100,factor=0.5,noise=0.1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к этому алгоритм опорных вектором с нелинейными ядерными функциями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.svm.SVC()\n",
    "model.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=['r' if x else 'b' for x in Y])\n",
    "\n",
    "ax = plt.gca()\n",
    "# строим разделяющие прямые\n",
    "sk.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "    model,X,plot_method=\"contour\",colors=\"k\",\n",
    "    levels=[-1, 0, 1],alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"], ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пример, как меняется точность классификации при использовании линейных и нелинейных моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "linmodel = sk.linear_model.LogisticRegression().fit(X_train,Y_train)\n",
    "\n",
    "print(f\"Linear accuracy = {sk.metrics.accuracy_score(Y_test,linmodel.predict(X_test))}\")\n",
    "print(f\"Non-lin SVM accuracy = {sk.metrics.accuracy_score(Y_test,model.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что мы можем добиться высокой точности модели, добавив вручную дополнительные признаки в виде квадратов к нашему датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.hstack([X_train,X_train*X_train])\n",
    "X_test_aug = np.hstack([X_test,X_test*X_test])\n",
    "\n",
    "linaugmodel = sk.linear_model.LogisticRegression().fit(X_train_aug,Y_train)\n",
    "\n",
    "print(f\"Augmented linear accuracy = {sk.metrics.accuracy_score(Y_test,linaugmodel.predict(X_test_aug))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такого же результата можно было добиться, используя трансформер `PolynomialFeatures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "\n",
    "pipe = sk.pipeline.Pipeline([\n",
    "    ('PolyTransformer',sk.preprocessing.PolynomialFeatures()),\n",
    "    ('LinearModel',sk.linear_model.LogisticRegression())\n",
    "])\n",
    "pipe.fit(X_train,Y_train)\n",
    "print(f\"Pipeline accuracy = {sk.metrics.accuracy_score(Y_test,pipe.predict(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мультиклассовая классификация\n",
    "\n",
    "В качестве задачи мультиклассовой классификации рассмотрим задачу распознавания рукописных цифр. Для этого используем датасет [MNIST](https://ru.wikipedia.org/wiki/MNIST_(%D0%B1%D0%B0%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)) - это датасет рукописных цифр, подготовленный национальным институтом стандартов США. Но содержит обучающий набор в 60000 рукописных цифр, собранный от 250 различных студентов и сотрудников института, а также тестовый набор из 10000 цифр, собранных от других сотрудников. Каждая цифра представляется матрицей 28x28 точек в 256 градациях серого.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(X.iloc[i].to_numpy().reshape(28,28))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(Y[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет состоит из 70000 рукописных цифр размерностью 28 на 28 пикселей, в 256 градациях яркости.\n",
    "\n",
    "Для начала, разобьем датасет на обучаующую и тестовую выборки. Для увеличения скорости будем использовать по 10000 цифр в обучающем и тестовом датасете. Вы можете попробовать использовать весь датасет, но в этом случае приготовьтесь ждать!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = \\\n",
    "  sk.model_selection.train_test_split(X,Y,train_size=10000, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотренные нами алгоритмы классификации были **бинарными**, поэтому для многоклассовой классификации можно использовать подход **один против всех**. Например, обучим классификатор для обнаружения цифры 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_train = Y_train==\"0\"\n",
    "Y0_test = Y_test==0\n",
    "model = sk.linear_model.LogisticRegression().fit(X_train,Y0_train)\n",
    "sk.metrics.accuracy_score(Y0_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим здесь важную проблему - алгоритм оптимизации предупреждает, что не удалось достичь сходимости. Это может быть вызвано двумя проблемами:\n",
    "\n",
    "* Используемый по умолчанию в `LogisticRegression` оптимизатор - это не совсем градиентный спуск, а алгоритм `lbfgs`, который хорошо работает на сравнительно небольших датасетах. Можно попробовать задать другой оптимизатор параметром `solver=...`.\n",
    "* Cогласно негласному соглашению, входные числовые признаки должны находиться в районе диапазона 0..1 - исходя из этого реализованы все алгоритмы внутри библиотек. Это особенно важно, поскольку во многих случаях признаки имеют различный диапазон (например, рост человека меняется от 30 до 250 см., в то время как возраст - от 0 до 120), и для более *честного* обучения имеет смысл приводить их к единому диапазону. Мы раньше не занимались приведением признаков к диапазону 0..1, поскольку мы не сталкивались с моделями большой размерности. Здесь же мы имеем 768 входов, и значения в интервале 0..255, которые сильно выходят за рамки общепринятого.\n",
    "\n",
    "Для борьбы с проблемой нормируем все значения в датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg').fit(X_train/255.0,Y0_train)\n",
    "sk.metrics.accuracy_score(Y0_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем точность около 90% на классификации \"ноль-не ноль\". Для многоклассовой классификации нам необходимо построить 10 таких моделей, по одной на каждую цифру. Sklearn содержит специальный класс, который позволяет превратить любой двоичный классификатор в мультиклассовый с помощью подхода **One vs. All**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.multiclass\n",
    "\n",
    "model = sk.multiclass.OneVsRestClassifier(\n",
    "    sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1))\n",
    "model.fit(X_train/255.0, Y_train)\n",
    "sk.metrics.accuracy_score(Y_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем посмотреть на матрицу ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.ConfusionMatrixDisplay.from_estimator(model,X_test/255.0,Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OneVsRestClassifier` может превратить любой бинарный классификатор в мультиклассовый. Однако многие классификаторы в SkLearn, включая логистическую регрессию, поддерживают мультиклассовый режим. Поэтому мы можем подавать мультиклассовые данные напрямую на вход `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1)\n",
    "model.fit(X_train/255.0, Y_train)\n",
    "sk.metrics.accuracy_score(Y_test,model.predict(X_test/255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно посмотреть на коэффициенты (веса) получившейся модели. В мультиклассовом случае, размерность вектора коэффициентов `model.coef_` будет $10\\times784$ - это соответствует 10 классам, по $768=28\\times28$ коэффициентов в каждом. Соответственно, можем попробовать визуализировать это как изображение: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,10,figsize=(15,6))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(model.coef_[i].reshape(28,28))\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из картинки становится примерно понятно, как работает такой классификатор - он умножает шаблон с коэффициентами на изображение, и выход получается тем больше, чем лучше шаблон совпадает с картинкой. Положительные коэффициенты в шаблоне соответствуют тем пикселям, которые должны присутствовать для данной цифры, а отрицательные - те, которые для такой цифры не характерны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация\n",
    "\n",
    "Для проверки точности модели мы обычно использовали подход с разбиением выборки на обучающую и тестовую. Однако в этом случае часть выборки \"теряется\" и не используется для обучения; кроме того - есть вероятность, что конкретное разбиение на обучающую и тестовую выборку оказалось каким-то \"неудачным\".\n",
    "\n",
    "Для преодоления этих сложностей может использоваться подход, называемый **кросс-валидацией** (*K-Fold Cross-Validation*). В этом случае датасет разделяется на $k$ частей, и проводится $k$ экспериментов, в которых одна из частей используется для валидации, а остальные - для обучения. Полученные результаты (точность) затем усредняются.\n",
    "\n",
    "Scikit Learn позволяет провести обучение с кросс-валидацией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92192857, 0.92128571, 0.91521429, 0.91828571, 0.92978571])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sk.linear_model.LogisticRegression(solver='newton-cg',tol=0.1)\n",
    "scores = sk.model_selection.cross_val_score(model,X/255.0,Y,cv=5) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9212999999999999, 0.004868767601681531)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе `cross_val_score` можно задавать различные метрики для оценки, а также различные стратегии разбиения датасета (с помощью параметра `cv`).\n",
    "\n",
    "Существует также метод `cross_val_predict`, возвращающий предсказания можели для каждого из элементов входного датасета в то время, когда он был частью тестовой выборки.\n",
    "\n",
    "Если же нам нужно вернуть сами обученные модели, полученные в процессе кросс-валидации, то можно использовать метод `cross_validate`. Подробнее об особенностях использования этого метода можно почитать [в документации](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d21c24a7952cc9fa93a33c3da71b373ffe761d6ab0c9cc9b2153e31da51ffb58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
